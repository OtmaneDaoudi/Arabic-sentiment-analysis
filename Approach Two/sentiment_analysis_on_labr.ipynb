{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up the environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarabic\n",
    "!pip install Arabic-Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABR dataset\n",
    "\n",
    "- LABR stands for Large-Scale Arabic Book Reviews.\n",
    "- It's a collection of over 63,000 book reviews written in Arabic.\n",
    "- Each review comes with a rating on a scale of 1 to 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arabicstopwords.arabicstopwords as stp\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import pyarabic.araby as araby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review.tsv file contains :\n",
    "\n",
    "- rating - review id - user id - book id - review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the tsv file\n",
    "SEED = 21\n",
    "\n",
    "dataset = pd.read_csv(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/reviews.tsv\", sep = '\\t', header=None, names = [\"rating\",\"review_id\",\"user_id\",\"book_id\",\"review\"])\n",
    "\n",
    "dataset = dataset.sample(frac=1, random_state = SEED)\n",
    "\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape of the dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "dataset.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the distribution of the ratings\n",
    "print(dataset['rating'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the distribution of the ratings\n",
    "print(dataset['rating'].value_counts(normalize=True).plot(kind='bar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the review_id, user_id and book_id columns\n",
    "dataset = dataset.drop(['review_id','user_id','book_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the duplicates and keeping the first occurence\n",
    "dataset = dataset.drop_duplicates(subset='review', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing whitespaces\n",
    "\n",
    "pattern = r'\\s+|\\n+'\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: re.sub(pattern, ' ', document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "\n",
    "pattern = r'[^\\w\\s\\u0600-\\u06FF]+|ﷺ|۩|⓵|؟|۞|ﷻ'\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: re.sub(pattern, '', document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing consecutive characters in arabic \n",
    "\n",
    "pattern = r'(.)\\1+'\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: re.sub(pattern, r'\\1', document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words \n",
    "\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "stop_words.update(stp.stopwords_list())\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: ' '.join([word for word in document.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing arabic diactrics\n",
    "\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: araby.strip_tashkeel(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing numbers \n",
    "\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: ''.join([i for i in document if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing english alphabets \n",
    "\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: re.sub(r'[a-zA-Z]+', '', document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the reviews using nltk \n",
    "\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: word_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming the reviews using nltk\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda document: [stemmer.stem(word) for word in document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the first 20 rows of the dataset\n",
    "\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the cleaned dataset\n",
    "\n",
    "dataset.to_csv(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/cleaned_reviews.tsv\", sep = '\\t', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the cleaned dataset\n",
    "\n",
    "dataset = pd.read_csv(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/cleaned_reviews.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train , X_test = train_test_split(dataset, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bag of words model\n",
    "\n",
    "cv = CountVectorizer(max_features=5000)\n",
    "X_train = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test = cv.transform(X_test['review']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the target variable\n",
    "\n",
    "y_train = dataset.iloc[X_train.index]['rating']\n",
    "y_test = dataset.iloc[X_test.index]['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the bag of words model\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/models/bag_of_words_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(cv, file)\n",
    "\n",
    "# saving the train and test sets\n",
    "\n",
    "np.save(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/X_train.npy\", X_train)\n",
    "np.save(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/X_test.npy\", X_test)\n",
    "np.save(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/y_train.npy\", y_train)\n",
    "np.save(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the bag of words model\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/models/bag_of_words_model.pkl\", \"rb\") as file:\n",
    "    cv = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the train and test sets\n",
    "\n",
    "X_train = np.load(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/X_train.npy\")\n",
    "X_test = np.load(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/X_test.npy\")\n",
    "\n",
    "y_train = np.load(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/y_train.npy\")\n",
    "y_test = np.load(\"/content/drive/MyDrive/Parcours Academique/ENSAM/PFA/datasets/LABR/y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# training the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# making predictions\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# calculating the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"The accuracy of the model is: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# training the model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# making predictions\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# calculating the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"The accuracy of the model is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# training the model\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# making predictions\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# calculating the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"The accuracy of the model is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# training the model\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# making predictions\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# calculating the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"The accuracy of the model is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train , X_test = train_test_split(dataset, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# creating the tf-idf model\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train['review']).toarray()\n",
    "\n",
    "X_test = tfidf.transform(X_test['review']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# training the model\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# making predictions\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# calculating the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"The accuracy of the model is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# training the model\n",
    "\n",
    "gnb = MultinomialNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# making predictions\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# calculating the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"The accuracy of the model is: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
