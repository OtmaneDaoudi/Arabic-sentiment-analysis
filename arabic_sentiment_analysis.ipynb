{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OtmaneDaoudi/Arabic-sentiment-analysis/blob/main/arabic_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YCMH2OjBGA8u"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BqPG4jsGF8m",
        "outputId": "71519abe-6ccb-4059-c163-edb45164bcbe"
      },
      "outputs": [],
      "source": [
        "# !pip install emoji\n",
        "# !pip install Arabic-Stopwords\n",
        "# !pip install seaborn\n",
        "# !pip install matplotlib\n",
        "# !pip install soyclustering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KUh0GecqEErv"
      },
      "source": [
        "# Libs imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQdYY3KPEEsC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "import emoji\n",
        "import arabicstopwords.arabicstopwords as stp\n",
        "import tqdm\n",
        "import os\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "\n",
        "from farasa.stemmer import FarasaStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "SEED = 21"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "MvI6l3cnEEsH",
        "outputId": "840f53a3-1120-4709-a395-10f90977c0a9"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data = pd.read_excel(\"datasets/AJGT/AJGT.xlsx\", header = 0, names = [\"tweet\", \"class\"])\n",
        "else:\n",
        "    data = pd.read_excel(\"preprocessed_data.xlsx\", header = 0, names = [\"tweet\", \"class\"])\n",
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqggWownEEsK",
        "outputId": "e8a398e4-bfd2-44cf-b85f-2ea0ab850aa8"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GIECLLDlEEsM"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NDtOpDzdEEsP"
      },
      "source": [
        "Our preprocessing pipeline contains the following steps:\n",
        "\n",
        "1.  Remove duplicat entries\n",
        "2.  Replacing emojies & emoticons\n",
        "3.  Remove mentions\n",
        "4.  Remove Links\n",
        "5.  Remove whitespaces\n",
        "6.  Remove punctuations & Special chars\n",
        "7.  Remove Consecutive characters\n",
        "8.  Tokenization\n",
        "9.  Remove foreign words\n",
        "10. Remove stop words\n",
        "11. Remove numbers\n",
        "12. Stemming\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zolKP03vEEsT"
      },
      "source": [
        "## Removing duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeMI9vGpEEsW",
        "outputId": "e2fb0124-2a17-4db2-c06f-52d48a0b7000"
      },
      "outputs": [],
      "source": [
        "count = data.duplicated().sum()\n",
        "print(f\"{(count / data.shape[0]) * 100:.1f}% of the data are duplicats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahlc7NfGEEsY"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data.drop_duplicates(inplace = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv54Q2aKEEsZ"
      },
      "source": [
        "## Replacing emojies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Dy9RLbEEsa"
      },
      "outputs": [],
      "source": [
        "emojis = {\n",
        "    \"ðŸ™‚\":\"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"ðŸ˜‚\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸ¤£\" : \"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸ’”\":\"Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†\",\n",
        "    \"ðŸ™‚\":\"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"â¤ï¸\":\"Ø­Ø¨\",\n",
        "    \"ðŸ¥°\":\"Ø­Ø¨\",\n",
        "    \"ðŸ¤\":\"Ø³ÙƒÙˆØª\",\n",
        "    \"ðŸ§¡\":\"Ø­Ø¨\",\n",
        "    \"â¤\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜­\":\"ÙŠØ¨ÙƒÙŠ\",\n",
        "    \"ðŸ¤­\":\"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"ðŸ˜¢\":\"Ø­Ø²Ù†\",\n",
        "    \"ðŸ˜”\":\"Ø­Ø²Ù†\",\n",
        "    \"â™¥\":\"Ø­Ø¨\",\n",
        "    \"ðŸ’œ\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜…\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸ™\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ðŸ’•\":\"Ø­Ø¨\",\n",
        "    \"ðŸ’™\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜ž\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ðŸ˜Š\":\"Ø³Ø¹Ø§Ø¯Ø©\",\n",
        "    \"ðŸ‘\":\"ÙŠØµÙÙ‚\",\n",
        "    \"ðŸ‘Œ\":\"Ø§Ø­Ø³Ù†Øª\",\n",
        "    \"ðŸ˜´\":\"ÙŠÙ†Ø§Ù…\",\n",
        "    \"ðŸ˜€\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"âœ…\":\"ØµØ­ÙŠØ­\",\n",
        "    \"ðŸ¤ª\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸ¡\" : \"Ø¨ÙŠØª\",\n",
        "    \"ðŸ¤²\" : \"Ø¯Ø¹Ø§Ø¡\",\n",
        "    \"ðŸ’°\" : \"Ù…Ø§Ù„\",\n",
        "    \"ðŸ˜Œ\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ðŸŽ\":\"Ù‡Ø¯ÙŠØ©\",\n",
        "    \"ðŸŒ¹\":\"ÙˆØ±Ø¯Ø©\",\n",
        "    \"ðŸ¥€\":\"ÙˆØ±Ø¯Ø©\",\n",
        "    \"ðŸ“¿\":\"ÙˆØ±Ø¯Ø©\",\n",
        "    \"âœ\":\"ÙƒØªØ§Ø¨Ø©\",\n",
        "    \"ðŸ™ˆ\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜„\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸ˜\":\"Ù…Ø­Ø§ÙŠØ¯\",\n",
        "    \"âœŒ\":\"Ù…Ù†ØªØµØ±\",\n",
        "    \"âœ¨\":\"Ù†Ø¬Ù…Ù‡\",\n",
        "    \"ðŸ¤”\":\"ØªÙÙƒÙŠØ±\",\n",
        "    \"ðŸ˜\":\"ÙŠØ³ØªÙ‡Ø²Ø¡\",\n",
        "    \"ðŸ˜’\":\"ÙŠØ³ØªÙ‡Ø²Ø¡\",\n",
        "    \"ðŸ™„\":\"Ù…Ù„Ù„\",\n",
        "    \"ðŸ˜•\":\"Ø¹ØµØ¨ÙŠØ©\",\n",
        "    \"ðŸ˜ƒ\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸŒ¸\":\"ÙˆØ±Ø¯Ø©\",\n",
        "    \"ðŸ˜“\":\"Ø­Ø²Ù†\",\n",
        "    \"ðŸ’ž\":\"Ø­Ø¨\",\n",
        "    \"ðŸ’—\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜‘\":\"Ù…Ù†Ø²Ø¹Ø¬\",\n",
        "    \"ðŸ’­\":\"ØªÙÙƒÙŠØ±\",\n",
        "    \"ðŸ˜Ž\":\"Ø«Ù‚Ø©\",\n",
        "    \"ðŸ’›\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜©\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ðŸ¥º\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ðŸ’ª\":\"Ø¹Ø¶Ù„Ø§Øª\",\n",
        "    \"ðŸ‘\":\"Ù…ÙˆØ§ÙÙ‚\",\n",
        "    \"ðŸ™ðŸ»\":\"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨\",\n",
        "    \"ðŸ˜³\":\"Ù…ØµØ¯ÙˆÙ…\",\n",
        "    \"ðŸ‘ðŸ¼\":\"ØªØµÙÙŠÙ‚\",\n",
        "    \"ðŸŽ¶\":\"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\",\n",
        "    \"ðŸŒš\":\"ØµÙ…Øª\",\n",
        "    \"ðŸ’š\":\"Ø­Ø¨\",\n",
        "    \"ðŸ™\":\"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨\",\n",
        "    \"ðŸ’˜\":\"Ø­Ø¨\",\n",
        "    \"ðŸƒ\":\"Ø³Ù„Ø§Ù…\",\n",
        "    \"â˜º\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸŽŠ\":\"ÙŠÙ‡Ù†Ø¦\",\n",
        "    \"ðŸ’¥\":\"Ø¥Ù†ÙØ¬Ø§Ø±\",\n",
        "    \"ðŸ˜\":\"ÙŠØ³Ø®Ø±\",\n",
        "    \"ðŸ’¯\":\"ØªÙ…Ø§Ù…\",\n",
        "    \"ðŸ¸\":\"Ø¶ÙØ¯Ø¹\",\n",
        "    \"ðŸ¤¦â€â™‚ï¸\":\"ØºØ¨ÙŠ\",\n",
        "    \"ðŸ¤©\":\"Ù…Ø¹Ø¬Ø¨\",\n",
        "    \"ðŸ¤¤\":\"Ø¬Ø§Ø¦Ø¹\",\n",
        "    \"ðŸ˜¶\":\"Ù…ØµØ¯ÙˆÙ…\",\n",
        "    \"âœŒï¸\":\"Ù…Ø±Ø­\",\n",
        "    \"âœ‹ðŸ»\":\"ØªÙˆÙ‚Ù\",\n",
        "    \"ðŸ˜‰\":\"ØºÙ…Ø²Ø©\",\n",
        "    \"ðŸŒ·\":\"Ø­Ø¨\",\n",
        "    \"ðŸ™ƒ\":\"Ù…Ø¨ØªØ³Ù…\",\n",
        "    \"ðŸ˜«\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ðŸ˜¨\":\"Ù…ØµØ¯ÙˆÙ…\",\n",
        "    \"ðŸŽ¼ \":\"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\",\n",
        "    \"ðŸ\":\"Ù…Ø±Ø­\",\n",
        "    \"ðŸ‚\":\"Ù…Ø±Ø­\",\n",
        "    \"ðŸ’Ÿ\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜ª\":\"Ø­Ø²Ù†\",\n",
        "    \"ðŸ˜†\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸ˜£\":\"Ø§Ø³ØªÙŠØ§Ø¡\",\n",
        "    \"â˜ºï¸\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜±\":\"ÙƒØ§Ø±Ø«Ø©\",\n",
        "    \"ðŸ˜\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ðŸ˜–\":\"Ø§Ø³ØªÙŠØ§Ø¡\",\n",
        "    \"ðŸƒðŸ¼\":\"ÙŠØ¬Ø±ÙŠ\",\n",
        "    \"ðŸ˜¡\":\"ØºØ¶Ø¨\",\n",
        "    \"ðŸš¶\":\"ÙŠØ³ÙŠØ±\",\n",
        "    \"ðŸ¤•\":\"Ù…Ø±Ø¶\",\n",
        "    \"ðŸ¤®\" : \"ÙŠØªÙ‚ÙŠØ¦\",\n",
        "    \"â›”\": \"Ø­Ø°Ø±\",\n",
        "    \"â€¼ï¸\":\"ØªØ¹Ø¬Ø¨\",\n",
        "    \"ðŸ•Š\":\"Ø·Ø§Ø¦Ø±\",\n",
        "    \"ðŸ‘ŒðŸ»\":\"Ø§Ø­Ø³Ù†Øª\",\n",
        "    \"â£\":\"Ø­Ø¨\",\n",
        "    \"ðŸ™Š\":\"Ù…ØµØ¯ÙˆÙ…\",\n",
        "    \"ðŸ’ƒ\":\"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­\",\n",
        "    \"ðŸ’ƒðŸ¼\":\"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­\",\n",
        "    \"ðŸ˜œ\":\"Ù…Ø±Ø­\",\n",
        "    \"ðŸ‘Š\":\"Ø¶Ø±Ø¨Ø©\",\n",
        "    \"ðŸ˜Ÿ\":\"Ø§Ø³ØªÙŠØ§Ø¡\",\n",
        "    \"ðŸ’–\":\"Ø­Ø¨\",\n",
        "    \"ðŸ˜¥\":\"Ø­Ø²Ù†\",\n",
        "    \"ðŸŽ»\":\"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\",\n",
        "    \"âœ’\":\"ÙŠÙƒØªØ¨\",\n",
        "    \"ðŸš¶ðŸ»\":\"ÙŠØ³ÙŠØ±\",\n",
        "    \"ðŸ’Ž\":\"Ø§Ù„Ù…Ø§Ø¸\",\n",
        "    \"ðŸ˜·\":\"ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶\",\n",
        "    \"â˜\":\"ÙˆØ§Ø­Ø¯\",\n",
        "    \"ðŸš¬\":\"ØªØ¯Ø®ÙŠÙ†\",\n",
        "    \"ðŸ’\" : \"ÙˆØ±Ø¯\",\n",
        "    \"ðŸŒ»\" : \"ÙˆØ±Ø¯\",\n",
        "    \"ðŸŒž\" : \"Ø´Ù…Ø³\",\n",
        "    \"ðŸ‘†\" : \"Ø§Ù„Ø§ÙˆÙ„\",\n",
        "    \"âš ï¸\" :\"ØªØ­Ø°ÙŠØ±\",\n",
        "    \"ðŸ¤—\" : \"Ø§Ø­ØªÙˆØ§Ø¡\",\n",
        "    \"âœ–ï¸\": \"ØºÙ„Ø·\",\n",
        "    \"ðŸ“\"  : \"Ù…ÙƒØ§Ù†\",\n",
        "    \"ðŸ‘¸\" : \"Ù…Ù„ÙƒÙ‡\",\n",
        "    \"ðŸ‘‘\" : \"ØªØ§Ø¬\",\n",
        "    \"âœ”ï¸\" : \"ØµØ­\",\n",
        "    \"ðŸ’Œ\": \"Ù‚Ù„Ø¨\",\n",
        "    \"ðŸ˜²\" : \"Ù…Ù†Ø¯Ù‡Ø´\",\n",
        "    \"ðŸ’¦\": \"Ù…Ø§Ø¡\",\n",
        "    \"ðŸš«\" : \"Ø®Ø·Ø§\",\n",
        "    \"ðŸ‘ðŸ»\" : \"Ø¨Ø±Ø§ÙÙˆ\",\n",
        "    \"ðŸŠ\" :\"ÙŠØ³Ø¨Ø­\",\n",
        "    \"ðŸ‘ðŸ»\": \"ØªÙ…Ø§Ù…\",\n",
        "    \"â­•ï¸\" :\"Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡\",\n",
        "    \"ðŸŽ·\" : \"Ø³Ø§ÙƒØ³ÙÙˆÙ†\",\n",
        "    \"ðŸ‘‹\": \"ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯\",\n",
        "    \"âœŒðŸ¼\": \"Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±\",\n",
        "    \"ðŸŒ\":\"Ù…Ø¨ØªØ³Ù…\",\n",
        "    \"âž¿\"  : \"Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡\",\n",
        "    \"ðŸ’ªðŸ¼\" : \"Ù‚ÙˆÙŠ\",\n",
        "    \"ðŸ“©\":  \"ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ\",\n",
        "    \"â˜•ï¸\": \"Ù‚Ù‡ÙˆÙ‡\",\n",
        "    \"ðŸ˜§\" : \"Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©\",\n",
        "    \"ðŸ—¨\": \"Ø±Ø³Ø§Ù„Ø©\",\n",
        "    \"â—ï¸\" :\"ØªØ¹Ø¬Ø¨\",\n",
        "    \"ðŸ™†ðŸ»\": \"Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡\",\n",
        "    \"ðŸ‘¯\" :\"Ø§Ø®ÙˆØ§Øª\",\n",
        "    \"Â©\" :  \"Ø±Ù…Ø²\",\n",
        "    \"ðŸ‘µðŸ½\" :\"Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡\",\n",
        "    \"ðŸ£\": \"ÙƒØªÙƒÙˆØª\",\n",
        "    \"ðŸ™Œ\": \"ØªØ´Ø¬ÙŠØ¹\",\n",
        "    \"ðŸ™‡\": \"Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ\",\n",
        "    \"ðŸ‘ðŸ½\":\"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡\",\n",
        "    \"ðŸ‘ŒðŸ½\": \"Ø¨Ø§Ù„Ø¸Ø¨Ø·\",\n",
        "    \"â‰ï¸\" : \"Ø§Ø³ØªÙ†ÙƒØ§Ø±\",\n",
        "    \"âš½ï¸\": \"ÙƒÙˆØ±Ù‡\",\n",
        "    \"ðŸ•¶\" :\"Ø­Ø¨\",\n",
        "    \"ðŸŽˆ\" :\"Ø¨Ø§Ù„ÙˆÙ†\",\n",
        "    \"ðŸŽ€\":    \"ÙˆØ±Ø¯Ù‡\",\n",
        "    \"ðŸ’µ\":  \"ÙÙ„ÙˆØ³\",\n",
        "    \"ðŸ˜‹\":  \"Ø¬Ø§Ø¦Ø¹\",\n",
        "    \"ðŸ˜›\":  \"ÙŠØºÙŠØ¸\",\n",
        "    \"ðŸ˜ \":  \"ØºØ§Ø¶Ø¨\",\n",
        "    \"âœðŸ»\":  \"ÙŠÙƒØªØ¨\",\n",
        "    \"ðŸŒ¾\":  \"Ø§Ø±Ø²\",\n",
        "    \"ðŸ‘£\":  \"Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ†\",\n",
        "    \"âŒ\":\"Ø±ÙØ¶\",\n",
        "    \"ðŸŸ\":\"Ø·Ø¹Ø§Ù…\",\n",
        "    \"ðŸ‘¬\":\"ØµØ¯Ø§Ù‚Ø©\",\n",
        "    \"ðŸ°\":\"Ø§Ø±Ù†Ø¨\",\n",
        "    \"ðŸ¦‹\" : \"ÙØ±Ø§Ø´Ø©\",\n",
        "    \"â˜‚\":\"Ù…Ø·Ø±\",\n",
        "    \"âšœ\":\"Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§\",\n",
        "    \"ðŸ‘\":\"Ø®Ø±ÙˆÙ\",\n",
        "    \"ðŸ—£\":\"ØµÙˆØª Ù…Ø±ØªÙØ¹\",\n",
        "    \"ðŸ‘ŒðŸ¼\":\"Ø§Ø­Ø³Ù†Øª\",\n",
        "    \"â˜˜\":\"Ù…Ø±Ø­\",\n",
        "    \"ðŸ˜®\":\"ØµØ¯Ù…Ø©\",\n",
        "    \"ðŸ˜¦\":\"Ù‚Ù„Ù‚\",\n",
        "    \"â­•\":\"Ø§Ù„Ø­Ù‚\",\n",
        "    \"âœï¸\":\"Ù‚Ù„Ù…\",\n",
        "    \"â„¹\":\"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\",\n",
        "    \"ðŸ™ðŸ»\":\"Ø±ÙØ¶\",\n",
        "    \"âšªï¸\":\"Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡\",\n",
        "    \"ðŸ¤\":\"Ø­Ø²Ù†\",\n",
        "    \"ðŸ’«\":\"Ù…Ø±Ø­\",\n",
        "    \"ðŸ’\":\"Ø­Ø¨\",\n",
        "    \"ðŸ”\":\"Ø·Ø¹Ø§Ù…\",\n",
        "    \"â¤ï¸Ž\":\"Ø­Ø¨\",\n",
        "    \"âœˆï¸\":\"Ø³ÙØ±\",\n",
        "    \"ðŸƒðŸ»â€â™€ï¸\":\"ÙŠØ³ÙŠØ±\",\n",
        "    \"ðŸ³\":\"Ø°ÙƒØ±\",\n",
        "    \"ðŸŽ¤\":\"Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡\",\n",
        "    \"ðŸŽ¾\":\"ÙƒØ±Ù‡\",\n",
        "    \"ðŸ”\":\"Ø¯Ø¬Ø§Ø¬Ø©\",\n",
        "    \"ðŸ™‹\":\"Ø³Ø¤Ø§Ù„\",\n",
        "    \"ðŸ“®\":\"Ø¨Ø­Ø±\",\n",
        "    \"ðŸ’‰\":\"Ø¯ÙˆØ§Ø¡\",\n",
        "    \"ðŸ™ðŸ¼\":\"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨\",\n",
        "    \"ðŸ’‚ðŸ¿ \":\"Ø­Ø§Ø±Ø³\",\n",
        "    \"ðŸŽ¬\":\"Ø³ÙŠÙ†Ù…Ø§\",\n",
        "    \"â™¦ï¸\":\"Ù…Ø±Ø­\",\n",
        "    \"ðŸ’¡\":\"Ù‚ÙƒØ±Ø©\",\n",
        "    \"â€¼\":\"ØªØ¹Ø¬Ø¨\",\n",
        "    \"ðŸ‘¼\":\"Ø·ÙÙ„\",\n",
        "    \"ðŸ”‘\":\"Ù…ÙØªØ§Ø­\",\n",
        "    \"â™¥ï¸\":\"Ø­Ø¨\",\n",
        "    \"ðŸŒ²\" : \"Ø´Ø¬Ø±Ø©\",\n",
        "    \"ðŸŒ³\" : \"Ø´Ø¬Ø±Ø©\",\n",
        "    \"ðŸš©\" : \"Ø­Ø°Ø±\",\n",
        "    \"ðŸš¨\" : \"Ø­Ø°Ø±\",\n",
        "    \"ðŸ›‘\" : \"Ø­Ø°Ø±\",\n",
        "    \"ðŸ•‹\":\"ÙƒØ¹Ø¨Ø©\",\n",
        "    \"ðŸ“\":\"Ø¯Ø¬Ø§Ø¬Ø©\",\n",
        "    \"ðŸ’©\":\"Ù…Ø¹ØªØ±Ø¶\",\n",
        "    \"ðŸ‘½\":\"ÙØ¶Ø§Ø¦ÙŠ\",\n",
        "    \"â˜”ï¸\":\"Ù…Ø·Ø±\",\n",
        "    \"ðŸ·\":\"Ø¹ØµÙŠØ±\",\n",
        "    \"ðŸŒŸ\":\"Ù†Ø¬Ù…Ø©\",\n",
        "    \"â˜ï¸\":\"Ø³Ø­Ø¨\",\n",
        "    \"ðŸ‘ƒ\":\"Ù…Ø¹ØªØ±Ø¶\",\n",
        "    \"ðŸŒº\":\"Ù…Ø±Ø­\",\n",
        "    \"ðŸ”ª\":\"Ø³ÙƒÙŠÙ†Ø©\",\n",
        "    \"â™¨\":\"Ø³Ø®ÙˆÙ†ÙŠØ©\",\n",
        "    \"ðŸ‘ŠðŸ¼\":\"Ø¶Ø±Ø¨\",\n",
        "    \"âœ\":\"Ù‚Ù„Ù…\",\n",
        "    \"ðŸš¶ðŸ¾â€â™€ï¸\":\"ÙŠØ³ÙŠØ±\",\n",
        "    \"ðŸ‘Š\":\"Ø¶Ø±Ø¨Ø©\",\n",
        "    \"â—¾ï¸\":\"ÙˆÙ‚Ù\",\n",
        "    \"ðŸ˜š\":\"Ø­Ø¨\",\n",
        "    \"ðŸ”¸\":\"Ù…Ø±Ø­\",\n",
        "    \"ðŸ‘ŽðŸ»\":\"Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ\",\n",
        "    \"ðŸ‘ŠðŸ½\":\"Ø¶Ø±Ø¨Ø©\",\n",
        "    \"ðŸ˜™\":\"Ø­Ø¨\",\n",
        "    \"ðŸŽ¥\":\"ØªØµÙˆÙŠØ±\",\n",
        "    \"ðŸ‘‰\":\"Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡\",\n",
        "    \"ðŸ‘ðŸ½\":\"ÙŠØµÙÙ‚\",\n",
        "    \"ðŸ’ªðŸ»\":\"Ø¹Ø¶Ù„Ø§Øª\",\n",
        "    \"ðŸ´\":\"Ø§Ø³ÙˆØ¯\",\n",
        "    \"ðŸ”¥\":\"Ø­Ø±ÙŠÙ‚\",\n",
        "    \"ðŸ˜¬\":\"Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©\",\n",
        "    \"ðŸ‘ŠðŸ¿\":\"ÙŠØ¶Ø±Ø¨\",\n",
        "    \"ðŸ“š\" : \"ÙƒØªØ¨\",\n",
        "    \"ðŸ“Œ\" : \"Ø¹Ù„Ù‚\",\n",
        "    \"ðŸŒ¿\":\"ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡\",\n",
        "    \"âœ‹ðŸ¼\":\"ÙƒÙ Ø§ÙŠØ¯\",\n",
        "    \"ðŸ‘\":\"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡\",\n",
        "    \"â˜ ï¸\":\"ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨\",\n",
        "    \"ðŸŽ‰\":\"ÙŠÙ‡Ù†Ø¦\",\n",
        "    \"ðŸ”•\" :\"ØµØ§Ù…Øª\",\n",
        "    \"ðŸ˜¿\":\"ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†\",\n",
        "    \"â˜¹ï¸\":\"ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³\",\n",
        "    \"ðŸ˜˜\" :\"Ø­Ø¨\",\n",
        "    \"ðŸ˜°\" :\"Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†\",\n",
        "    \"ðŸŒ¼\":\"ÙˆØ±Ø¯Ù‡\",\n",
        "    \"ðŸ’‹\": \"Ø¨ÙˆØ³Ù‡\",\n",
        "    \"ðŸ‘‡\":\"Ù„Ø§Ø³ÙÙ„\",\n",
        "    \"â£ï¸\":\"Ø­Ø¨\",\n",
        "    \"ðŸŽ§\":\"Ø³Ù…Ø§Ø¹Ø§Øª\",\n",
        "    \"ðŸ“\":\"ÙŠÙƒØªØ¨\",\n",
        "    \"ðŸ˜‡\":\"Ø¯Ø§ÙŠØ®\",\n",
        "    \"ðŸ˜ˆ\":\"Ø±Ø¹Ø¨\",\n",
        "    \"ðŸƒ\":\"ÙŠØ¬Ø±ÙŠ\",\n",
        "    \"âœŒðŸ»\":\"Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±\",\n",
        "    \"ðŸ”«\":\"ÙŠØ¶Ø±Ø¨\",\n",
        "    \"â—ï¸\":\"ØªØ¹Ø¬Ø¨\",\n",
        "    \"ðŸ‘Ž\":\"ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚\",\n",
        "    \"ðŸ”\":\"Ù‚ÙÙ„\",\n",
        "    \"ðŸ‘ˆ\":\"Ù„Ù„ÙŠÙ…ÙŠÙ†\",\n",
        "    \"â„¢\":\"Ø±Ù…Ø²\",\n",
        "    \"ðŸš¶ðŸ½\":\"ÙŠØªÙ…Ø´ÙŠ\",\n",
        "    \"ðŸ˜¯\":\"Ù…ØªÙØ§Ø¬Ø£\",\n",
        "    \"âœŠ\":\"ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡\",\n",
        "    \"ðŸ˜»\":\"Ø§Ø¹Ø¬Ø§Ø¨\",\n",
        "    \"ðŸ™‰\" :\"Ù‚Ø±Ø¯\",\n",
        "    \"ðŸ‘§\":\"Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡\",\n",
        "    \"ðŸ”´\":\"Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡\",\n",
        "    \"ðŸ’ªðŸ½\":\"Ù‚ÙˆÙ‡\",\n",
        "    \"ðŸ’¤\":\"ÙŠÙ†Ø§Ù…\",\n",
        "    \"ðŸ‘€\":\"ÙŠÙ†Ø¸Ø±\",\n",
        "    \"âœðŸ»\":\"ÙŠÙƒØªØ¨\",\n",
        "    \"â„ï¸\":\"ØªÙ„Ø¬\",\n",
        "    \"ðŸ’€\":\"Ø±Ø¹Ø¨\",\n",
        "    \"ðŸ˜¤\":\"ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³\",\n",
        "    \"ðŸ–‹\":\"Ù‚Ù„Ù…\",\n",
        "    \"ðŸŽ©\":\"ÙƒØ§Ø¨\",\n",
        "    \"â˜•ï¸\":\"Ù‚Ù‡ÙˆÙ‡\",\n",
        "    \"ðŸ˜¹\":\"Ø¶Ø­Ùƒ\",\n",
        "    \"ðŸ’“\":\"Ø­Ø¨\",\n",
        "    \"â˜„ï¸\":\"Ù†Ø§Ø±\",\n",
        "    \"ðŸ‘»\":\"Ø±Ø¹Ø¨\",\n",
        "    \"âœ‹\": \"ÙŠØ¯\",\n",
        "    \"ðŸŒ±\": \"Ù†Ø¨ØªØ©\",\n",
        "\n",
        "    # Emoticons\n",
        "    \":)\" : \"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"(:\" : \"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \":(\" : \"Ø­Ø²ÙŠÙ†\",\n",
        "    \"xD\" : \"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \":=(\": \"ÙŠØ¨ÙƒÙŠ\",\n",
        "    \":'(\": \"Ø­Ø²Ù†\",\n",
        "    \":'â€‘(\": \"Ø­Ø²Ù†\",\n",
        "    \"XD\" : \"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \":D\" : \"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"â™¬\" : \"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\",\n",
        "    \"â™¡\" : \"Ø­Ø¨\",\n",
        "    \"â˜»\"  : \"ÙŠØ¨ØªØ³Ù…\",\n",
        "}\n",
        "\n",
        "def replace_emojis(text):\n",
        "    pattern = re.compile('|'.join(re.escape(key) for key in emojis.keys()))\n",
        "    replaced_text = pattern.sub(lambda match: emojis[match.group(0)] + ' ', text)\n",
        "    return emoji.replace_emoji(replaced_text, '')\n",
        "\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: replace_emojis(document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ir8Glj9EEsf"
      },
      "source": [
        "## Removing mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXgrvqiKEEsg"
      },
      "outputs": [],
      "source": [
        "pattern = r'@[\\w]+'\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, '', document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AA7faVLeEEsh"
      },
      "source": [
        "## Removing links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm75bZvGEEsh"
      },
      "outputs": [],
      "source": [
        "pattern = r'https?://\\S+'\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, '', document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uNb_LQfGEEsl"
      },
      "source": [
        "## Removing whitespaces\n",
        "In this step we get rid of extra whitespaces as well as new lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ-mpCmREEsm"
      },
      "outputs": [],
      "source": [
        "pattern = r'\\s+|\\n+'\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMg71jAEEsn"
      },
      "source": [
        "## Remove foriegn words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1EjRxMEEsn"
      },
      "source": [
        "The text includes english, japanese and words for other languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HraPS5_SEEso"
      },
      "outputs": [],
      "source": [
        "pattern = r'[a-zA-Z]+'\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, '', document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_r3f6p9DEEsp"
      },
      "source": [
        "## Remove punctuations & special chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2gtB69BEEsp"
      },
      "outputs": [],
      "source": [
        "pattern = r'[^\\w\\s\\u0600-\\u06FF]+|ï·º|Û©|â“µ|ØŸ|Ø›|Ûž|ï·»'\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, '', document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-BEPhEhYEEsq"
      },
      "source": [
        "## Remove consecutive characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjEAl55ZEEsr"
      },
      "outputs": [],
      "source": [
        "pattern = r'(.)\\1+'\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, r'\\1', document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MZctqr8qEEsr"
      },
      "source": [
        "## Removing numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUfiKGlxEEst"
      },
      "outputs": [],
      "source": [
        "pattern = r'\\d+'\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, '', document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sy4GggUEEsw"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nf_D_MeEEsx"
      },
      "outputs": [],
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: tokenizer.tokenize(document))\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: \" \".join(document).strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "OK4X1pWUEEsx",
        "outputId": "dcd9960c-15d8-4ec5-e197-94b11c2cb673"
      },
      "outputs": [],
      "source": [
        "data.tail(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pQOiy36-EEsy"
      },
      "source": [
        "## Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFgkKc6GEEsy"
      },
      "outputs": [],
      "source": [
        "stemmer = FarasaStemmer()\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    with tqdm.tqdm(range(data.shape[0])) as progress:\n",
        "        for i in progress:\n",
        "            data.iloc[i, 0] = stemmer.stem(data.iloc[i, 0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arabic_stopwords = stopwords.words('arabic')\n",
        "arabic_stopwords.extend(stp.stopwords_list())\n",
        "stop_words = {entry for entry in arabic_stopwords}\n",
        "stopwords_stemmer = ISRIStemmer()\n",
        "with open(\"arabic_stopwords.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    for word in file:\n",
        "        pass\n",
        "        stop_words.add(stopwords_stemmer.stem(word.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_stop_words(tweet: str) -> str:\n",
        "    words = set(tweet.split(sep = ' '))\n",
        "    return \" \".join(list((words - stop_words)))\n",
        "\n",
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    data[\"tweet\"] = data[\"tweet\"].apply(lambda document: remove_stop_words(document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw0Z0MsXEEsz"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"preprocessed_data.xlsx\"):\n",
        "    # remove empty entries\n",
        "    data.replace('', pd.NA, inplace=True)  # Replace empty strings with NA\n",
        "    data.dropna(inplace=True)  # Drop rows with NA values\n",
        "    data.to_excel(\"preprocessed_data.xlsx\") # inspect the resulting file to validate the preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l4AEbbxhEEsz"
      },
      "source": [
        "# Text representation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BoW (Bag-of-Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnE8RuVGEEtI"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify = data[\"class\"])\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "X_train[0].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_N8hCmIEEtK",
        "outputId": "7fd72e2e-c809-445a-851d-035c5e8381eb"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(random_state = SEED)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM\n",
        "\n",
        "We were unable to train this model on our machines using the initial dataset, due to the **curse of dimensionality**, so we added dimensioanlity reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm = SVC(random_state = SEED)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(random_state = SEED)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify= data[\"class\"])\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "X_train[0].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify = data[\"class\"])\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components = 170, random_state = SEED)\n",
        "lda.fit(X_train)\n",
        "X_train = lda.transform(X_train)\n",
        "X_test = lda.transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 70.87%\n",
            "Percesion : 77.21%\n",
            "Recall : 58.99%\n",
            "F1 score : 66.88%\n"
          ]
        }
      ],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 72.27%\n",
            "Percesion : 74.53%\n",
            "Recall : 67.42%\n",
            "F1 score : 70.80%\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 71.99%\n",
            "Percesion : 75.32%\n",
            "Recall : 65.17%\n",
            "F1 score : 69.88%\n"
          ]
        }
      ],
      "source": [
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 66.39%\n",
            "Percesion : 64.95%\n",
            "Recall : 70.79%\n",
            "F1 score : 67.74%\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify = data[\"class\"])\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "lsa = TruncatedSVD(n_components = 200, random_state = SEED)\n",
        "X_train = lsa.fit_transform(X_train)\n",
        "X_test = lsa.transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 54.90%\n",
            "Percesion : 52.63%\n",
            "Recall : 95.51%\n",
            "F1 score : 67.86%\n"
          ]
        }
      ],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 78.71%\n",
            "Percesion : 82.28%\n",
            "Recall : 73.03%\n",
            "F1 score : 77.38%\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 76.19%\n",
            "Percesion : 81.21%\n",
            "Recall : 67.98%\n",
            "F1 score : 74.01%\n"
          ]
        }
      ],
      "source": [
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 79.83%\n",
            "Percesion : 79.44%\n",
            "Recall : 80.34%\n",
            "F1 score : 79.89%\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"Positive\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BoC (Bag-of-concepts)\n",
        "\n",
        "The BOC (Bag-of-Concepts) method has been proposed as a solution to the problem of large dimensions and sparsity that traditional methods such as TF-IDF and Bag of words suffer from."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word embeddings\n",
        "This is done using the AraVec model which is trained on arabic tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec.load(\"./aravec/tweets_cbow_300\")\n",
        "word_vecs = {}\n",
        "for tweet in data[\"tweet\"]:\n",
        "    for word in tweet.split(\" \"):\n",
        "        try:\n",
        "            word_vecs[word] = model.wv[word]\n",
        "        except Exception:\n",
        "            pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering the words embeddings\n",
        "This is done in order to extract concepts, using a variant of KMeans called spherical KMeans which uses cosine similarity, making it well suited for this task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_CONCEPTS = 80\n",
        "\n",
        "model = KMeans(n_clusters = NUM_CONCEPTS)\n",
        "X = list(word_vecs.values())\n",
        "model.fit(X)\n",
        "concepts = model.predict(X)\n"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concept extraction\n",
        "Now that we related words to concepts, we can create a document representataion, in which we express the degree of which a document contains a certain concept, and instead of taking only the freuqncies, we consider an approach similar to TF-IDF called, CF-IDF.\n",
        "\n",
        "CF-IDF is defined using the following formula : \n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "such that : \n",
        "\n",
        "    * |D| is the number of documents in the corpus\n",
        "    * n_c is the number of occurences of concept c in document d\n",
        "    * n_k is the total number of concepts in this document "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_DOCS = data.shape[0]\n",
        "\n",
        "# construct a word to concept mapping\n",
        "word_concept = {}\n",
        "for index, word in enumerate(word_vecs.keys()):\n",
        "    word_concept[word] = concepts[index]\n",
        "print(word_concept)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# construct a concept to document count mapping\n",
        "concept_docs = defaultdict(int)\n",
        "for doc in data[\"tweet\"]:\n",
        "    doc_concepts = set()\n",
        "    for word in doc.split(\" \"):\n",
        "        try:\n",
        "            doc_concepts.add(word_concept[word])\n",
        "        except Exception:\n",
        "            pass\n",
        "    for concept in doc_concepts:\n",
        "        concept_docs[concept] += 1\n",
        "print(concept_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cf_idf(document: str):\n",
        "    \"\"\" Returns the CD-IDF representataion of a document \"\"\"\n",
        "    res = [0 for _ in range(NUM_CONCEPTS)]\n",
        "    concepts_counts = defaultdict(int)\n",
        "    for word in document.split(\" \"):\n",
        "        try:\n",
        "            concepts_counts[word_concept[word]] += 1\n",
        "        except:\n",
        "            pass\n",
        "    n_k = sum(concepts_counts.values()) # number of concepts present in the document (duplicates are considered!)\n",
        "    for concept in range(NUM_CONCEPTS):\n",
        "        if concepts_counts[concept] != 0:\n",
        "            res[concept] = (concepts_counts[concept] / n_k) * log(NUM_DOCS / (1 + concept_docs[concept]))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = [cf_idf(tweet) for tweet in data[\"tweet\"]]\n",
        "y = data[\"class\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = SEED, stratify = y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance evaluation\n",
        "### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GaussianNB()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter = 1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
