{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OtmaneDaoudi/Arabic-sentiment-analysis/blob/main/arabic_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YCMH2OjBGA8u"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BqPG4jsGF8m",
        "outputId": "71519abe-6ccb-4059-c163-edb45164bcbe"
      },
      "outputs": [],
      "source": [
        "# !pip install emoji\n",
        "# !pip install Arabic-Stopwords\n",
        "# !pip install seaborn\n",
        "# !pip install matplotlib\n",
        "# !pip install soyclustering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KUh0GecqEErv"
      },
      "source": [
        "# Libs imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQdYY3KPEEsC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "import pickle\n",
        "\n",
        "import arabicstopwords.arabicstopwords as stp\n",
        "import pandas as pd\n",
        "import pyarabic.araby as araby\n",
        "import numpy as np \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "from snowballstemmer import stemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from typing import List\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "SEED = 21"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "MvI6l3cnEEsH",
        "outputId": "840f53a3-1120-4709-a395-10f90977c0a9"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"./datasets/ASTC/data.tsv\", header = 0, sep='\\t', names = [\"class\", \"tweet\"]).sample(frac = 1, random_state = SEED)\n",
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqggWownEEsK",
        "outputId": "e8a398e4-bfd2-44cf-b85f-2ea0ab850aa8"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GIECLLDlEEsM"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NDtOpDzdEEsP"
      },
      "source": [
        "Our preprocessing pipeline contains the following steps:\n",
        "\n",
        "1.  Remove duplicat entries\n",
        "2.  Replacing emojies & emoticons\n",
        "3.  Remove mentions\n",
        "4.  Remove Links\n",
        "5.  Remove whitespaces\n",
        "6.  Remove punctuations & Special chars\n",
        "7.  Remove Consecutive characters\n",
        "8.  Tokenization\n",
        "9.  Remove foreign words\n",
        "10. Remove stop words\n",
        "11. Remove numbers\n",
        "12. Stemming\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zolKP03vEEsT"
      },
      "source": [
        "## Removing duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeMI9vGpEEsW",
        "outputId": "e2fb0124-2a17-4db2-c06f-52d48a0b7000"
      },
      "outputs": [],
      "source": [
        "count = data.duplicated().sum()\n",
        "print(f\"{(count / data.shape[0]) * 100:.1f}% of the data are duplicats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahlc7NfGEEsY"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(inplace = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv54Q2aKEEsZ"
      },
      "source": [
        "## Replacing emojies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Dy9RLbEEsa"
      },
      "outputs": [],
      "source": [
        "emojis = {\n",
        "    \"🙂\":\"يبتسم\",\n",
        "    \"😂\":\"يضحك\",\n",
        "    \"🤣\" : \"يضحك\",\n",
        "    \"💔\":\"قلب حزين\",\n",
        "    \"🙂\":\"يبتسم\",\n",
        "    \"❤️\":\"حب\",\n",
        "    \"🥰\":\"حب\",\n",
        "    \"🤐\":\"سكوت\",\n",
        "    \"🧡\":\"حب\",\n",
        "    \"❤\":\"حب\",\n",
        "    \"😍\":\"حب\",\n",
        "    \"😭\":\"يبكي\",\n",
        "    \"🤭\":\"يبتسم\",\n",
        "    \"😢\":\"حزن\",\n",
        "    \"😔\":\"حزن\",\n",
        "    \"♥\":\"حب\",\n",
        "    \"💜\":\"حب\",\n",
        "    \"😅\":\"يضحك\",\n",
        "    \"🙁\":\"حزين\",\n",
        "    \"💕\":\"حب\",\n",
        "    \"💙\":\"حب\",\n",
        "    \"😞\":\"حزين\",\n",
        "    \"😊\":\"سعادة\",\n",
        "    \"👏\":\"يصفق\",\n",
        "    \"👌\":\"احسنت\",\n",
        "    \"😴\":\"ينام\",\n",
        "    \"😀\":\"يضحك\",\n",
        "    \"✅\":\"صحيح\",\n",
        "    \"🤪\":\"يضحك\",\n",
        "    \"🏡\" : \"بيت\",\n",
        "    \"🤲\" : \"دعاء\",\n",
        "    \"💰\" : \"مال\",\n",
        "    \"😌\":\"حزين\",\n",
        "    \"🎁\":\"هدية\",\n",
        "    \"🌹\":\"وردة\",\n",
        "    \"🥀\":\"وردة\",\n",
        "    \"📿\":\"وردة\",\n",
        "    \"✍\":\"كتابة\",\n",
        "    \"🙈\":\"حب\",\n",
        "    \"😄\":\"يضحك\",\n",
        "    \"😐\":\"محايد\",\n",
        "    \"✌\":\"منتصر\",\n",
        "    \"✨\":\"نجمه\",\n",
        "    \"🤔\":\"تفكير\",\n",
        "    \"😏\":\"يستهزء\",\n",
        "    \"😒\":\"يستهزء\",\n",
        "    \"🙄\":\"ملل\",\n",
        "    \"😕\":\"عصبية\",\n",
        "    \"😃\":\"يضحك\",\n",
        "    \"🌸\":\"وردة\",\n",
        "    \"😓\":\"حزن\",\n",
        "    \"💞\":\"حب\",\n",
        "    \"💗\":\"حب\",\n",
        "    \"😑\":\"منزعج\",\n",
        "    \"💭\":\"تفكير\",\n",
        "    \"😎\":\"ثقة\",\n",
        "    \"💛\":\"حب\",\n",
        "    \"😩\":\"حزين\",\n",
        "    \"🥺\":\"حزين\",\n",
        "    \"💪\":\"عضلات\",\n",
        "    \"👍\":\"موافق\",\n",
        "    \"🙏🏻\":\"رجاء طلب\",\n",
        "    \"😳\":\"مصدوم\",\n",
        "    \"👏🏼\":\"تصفيق\",\n",
        "    \"🎶\":\"موسيقي\",\n",
        "    \"🌚\":\"صمت\",\n",
        "    \"💚\":\"حب\",\n",
        "    \"🙏\":\"رجاء طلب\",\n",
        "    \"💘\":\"حب\",\n",
        "    \"🍃\":\"سلام\",\n",
        "    \"☺\":\"يضحك\",\n",
        "    \"🎊\":\"يهنئ\",\n",
        "    \"💥\":\"إنفجار\",\n",
        "    \"😝\":\"يسخر\",\n",
        "    \"💯\":\"تمام\",\n",
        "    \"🐸\":\"ضفدع\",\n",
        "    \"🤦‍♂️\":\"غبي\",\n",
        "    \"🤩\":\"معجب\",\n",
        "    \"🤤\":\"جائع\",\n",
        "    \"😶\":\"مصدوم\",\n",
        "    \"✌️\":\"مرح\",\n",
        "    \"✋🏻\":\"توقف\",\n",
        "    \"😉\":\"غمزة\",\n",
        "    \"🌷\":\"حب\",\n",
        "    \"🙃\":\"مبتسم\",\n",
        "    \"😫\":\"حزين\",\n",
        "    \"😨\":\"مصدوم\",\n",
        "    \"🎼 \":\"موسيقي\",\n",
        "    \"🍁\":\"مرح\",\n",
        "    \"🍂\":\"مرح\",\n",
        "    \"💟\":\"حب\",\n",
        "    \"😪\":\"حزن\",\n",
        "    \"😆\":\"يضحك\",\n",
        "    \"😣\":\"استياء\",\n",
        "    \"☺️\":\"حب\",\n",
        "    \"😱\":\"كارثة\",\n",
        "    \"😁\":\"يضحك\",\n",
        "    \"😖\":\"استياء\",\n",
        "    \"🏃🏼\":\"يجري\",\n",
        "    \"😡\":\"غضب\",\n",
        "    \"🚶\":\"يسير\",\n",
        "    \"🤕\":\"مرض\",\n",
        "    \"🤮\" : \"يتقيئ\",\n",
        "    \"⛔\": \"حذر\",\n",
        "    \"‼️\":\"تعجب\",\n",
        "    \"🕊\":\"طائر\",\n",
        "    \"👌🏻\":\"احسنت\",\n",
        "    \"❣\":\"حب\",\n",
        "    \"🙊\":\"مصدوم\",\n",
        "    \"💃\":\"سعادة مرح\",\n",
        "    \"💃🏼\":\"سعادة مرح\",\n",
        "    \"😜\":\"مرح\",\n",
        "    \"👊\":\"ضربة\",\n",
        "    \"😟\":\"استياء\",\n",
        "    \"💖\":\"حب\",\n",
        "    \"😥\":\"حزن\",\n",
        "    \"🎻\":\"موسيقي\",\n",
        "    \"✒\":\"يكتب\",\n",
        "    \"🚶🏻\":\"يسير\",\n",
        "    \"💎\":\"الماظ\",\n",
        "    \"😷\":\"وباء مرض\",\n",
        "    \"☝\":\"واحد\",\n",
        "    \"🚬\":\"تدخين\",\n",
        "    \"💐\" : \"ورد\",\n",
        "    \"🌻\" : \"ورد\",\n",
        "    \"🌞\" : \"شمس\",\n",
        "    \"👆\" : \"الاول\",\n",
        "    \"⚠️\" :\"تحذير\",\n",
        "    \"🤗\" : \"احتواء\",\n",
        "    \"✖️\": \"غلط\",\n",
        "    \"📍\"  : \"مكان\",\n",
        "    \"👸\" : \"ملكه\",\n",
        "    \"👑\" : \"تاج\",\n",
        "    \"✔️\" : \"صح\",\n",
        "    \"💌\": \"قلب\",\n",
        "    \"😲\" : \"مندهش\",\n",
        "    \"💦\": \"ماء\",\n",
        "    \"🚫\" : \"خطا\",\n",
        "    \"👏🏻\" : \"برافو\",\n",
        "    \"🏊\" :\"يسبح\",\n",
        "    \"👍🏻\": \"تمام\",\n",
        "    \"⭕️\" :\"دائره كبيره\",\n",
        "    \"🎷\" : \"ساكسفون\",\n",
        "    \"👋\": \"تلويح باليد\",\n",
        "    \"✌🏼\": \"علامه النصر\",\n",
        "    \"🌝\":\"مبتسم\",\n",
        "    \"➿\"  : \"عقده مزدوجه\",\n",
        "    \"💪🏼\" : \"قوي\",\n",
        "    \"📩\":  \"تواصل معي\",\n",
        "    \"☕️\": \"قهوه\",\n",
        "    \"😧\" : \"قلق و صدمة\",\n",
        "    \"🗨\": \"رسالة\",\n",
        "    \"❗️\" :\"تعجب\",\n",
        "    \"🙆🏻\": \"اشاره موافقه\",\n",
        "    \"👯\" :\"اخوات\",\n",
        "    \"©\" :  \"رمز\",\n",
        "    \"👵🏽\" :\"سيده عجوزه\",\n",
        "    \"🐣\": \"كتكوت\",\n",
        "    \"🙌\": \"تشجيع\",\n",
        "    \"🙇\": \"شخص ينحني\",\n",
        "    \"👐🏽\":\"ايدي مفتوحه\",\n",
        "    \"👌🏽\": \"بالظبط\",\n",
        "    \"⁉️\" : \"استنكار\",\n",
        "    \"⚽️\": \"كوره\",\n",
        "    \"🕶\" :\"حب\",\n",
        "    \"🎈\" :\"بالون\",\n",
        "    \"🎀\":    \"ورده\",\n",
        "    \"💵\":  \"فلوس\",\n",
        "    \"😋\":  \"جائع\",\n",
        "    \"😛\":  \"يغيظ\",\n",
        "    \"😠\":  \"غاضب\",\n",
        "    \"✍🏻\":  \"يكتب\",\n",
        "    \"🌾\":  \"ارز\",\n",
        "    \"👣\":  \"اثر قدمين\",\n",
        "    \"❌\":\"رفض\",\n",
        "    \"🍟\":\"طعام\",\n",
        "    \"👬\":\"صداقة\",\n",
        "    \"🐰\":\"ارنب\",\n",
        "    \"🦋\" : \"فراشة\",\n",
        "    \"☂\":\"مطر\",\n",
        "    \"⚜\":\"مملكة فرنسا\",\n",
        "    \"🐑\":\"خروف\",\n",
        "    \"🗣\":\"صوت مرتفع\",\n",
        "    \"👌🏼\":\"احسنت\",\n",
        "    \"☘\":\"مرح\",\n",
        "    \"😮\":\"صدمة\",\n",
        "    \"😦\":\"قلق\",\n",
        "    \"⭕\":\"الحق\",\n",
        "    \"✏️\":\"قلم\",\n",
        "    \"ℹ\":\"معلومات\",\n",
        "    \"🙍🏻\":\"رفض\",\n",
        "    \"⚪️\":\"نضارة نقاء\",\n",
        "    \"🐤\":\"حزن\",\n",
        "    \"💫\":\"مرح\",\n",
        "    \"💝\":\"حب\",\n",
        "    \"🍔\":\"طعام\",\n",
        "    \"❤︎\":\"حب\",\n",
        "    \"✈️\":\"سفر\",\n",
        "    \"🏃🏻‍♀️\":\"يسير\",\n",
        "    \"🍳\":\"ذكر\",\n",
        "    \"🎤\":\"مايك غناء\",\n",
        "    \"🎾\":\"كره\",\n",
        "    \"🐔\":\"دجاجة\",\n",
        "    \"🙋\":\"سؤال\",\n",
        "    \"📮\":\"بحر\",\n",
        "    \"💉\":\"دواء\",\n",
        "    \"🙏🏼\":\"رجاء طلب\",\n",
        "    \"💂🏿 \":\"حارس\",\n",
        "    \"🎬\":\"سينما\",\n",
        "    \"♦️\":\"مرح\",\n",
        "    \"💡\":\"قكرة\",\n",
        "    \"‼\":\"تعجب\",\n",
        "    \"👼\":\"طفل\",\n",
        "    \"🔑\":\"مفتاح\",\n",
        "    \"♥️\":\"حب\",\n",
        "    \"🌲\" : \"شجرة\",\n",
        "    \"🌳\" : \"شجرة\",\n",
        "    \"🚩\" : \"حذر\",\n",
        "    \"🚨\" : \"حذر\",\n",
        "    \"🛑\" : \"حذر\",\n",
        "    \"🕋\":\"كعبة\",\n",
        "    \"🐓\":\"دجاجة\",\n",
        "    \"💩\":\"معترض\",\n",
        "    \"👽\":\"فضائي\",\n",
        "    \"☔️\":\"مطر\",\n",
        "    \"🍷\":\"عصير\",\n",
        "    \"🌟\":\"نجمة\",\n",
        "    \"☁️\":\"سحب\",\n",
        "    \"👃\":\"معترض\",\n",
        "    \"🌺\":\"مرح\",\n",
        "    \"🔪\":\"سكينة\",\n",
        "    \"♨\":\"سخونية\",\n",
        "    \"👊🏼\":\"ضرب\",\n",
        "    \"✏\":\"قلم\",\n",
        "    \"🚶🏾‍♀️\":\"يسير\",\n",
        "    \"👊\":\"ضربة\",\n",
        "    \"◾️\":\"وقف\",\n",
        "    \"😚\":\"حب\",\n",
        "    \"🔸\":\"مرح\",\n",
        "    \"👎🏻\":\"لا يعجبني\",\n",
        "    \"👊🏽\":\"ضربة\",\n",
        "    \"😙\":\"حب\",\n",
        "    \"🎥\":\"تصوير\",\n",
        "    \"👉\":\"جذب انتباه\",\n",
        "    \"👏🏽\":\"يصفق\",\n",
        "    \"💪🏻\":\"عضلات\",\n",
        "    \"🏴\":\"اسود\",\n",
        "    \"🔥\":\"حريق\",\n",
        "    \"😬\":\"عدم الراحة\",\n",
        "    \"👊🏿\":\"يضرب\",\n",
        "    \"📚\" : \"كتب\",\n",
        "    \"📌\" : \"علق\",\n",
        "    \"🌿\":\"ورقه شجره\",\n",
        "    \"✋🏼\":\"كف ايد\",\n",
        "    \"👐\":\"ايدي مفتوحه\",\n",
        "    \"☠️\":\"وجه مرعب\",\n",
        "    \"🎉\":\"يهنئ\",\n",
        "    \"🔕\" :\"صامت\",\n",
        "    \"😿\":\"وجه حزين\",\n",
        "    \"☹️\":\"وجه يائس\",\n",
        "    \"😘\" :\"حب\",\n",
        "    \"😰\" :\"خوف و حزن\",\n",
        "    \"🌼\":\"ورده\",\n",
        "    \"💋\": \"بوسه\",\n",
        "    \"👇\":\"لاسفل\",\n",
        "    \"❣️\":\"حب\",\n",
        "    \"🎧\":\"سماعات\",\n",
        "    \"📝\":\"يكتب\",\n",
        "    \"😇\":\"دايخ\",\n",
        "    \"😈\":\"رعب\",\n",
        "    \"🏃\":\"يجري\",\n",
        "    \"✌🏻\":\"علامه النصر\",\n",
        "    \"🔫\":\"يضرب\",\n",
        "    \"❗️\":\"تعجب\",\n",
        "    \"👎\":\"غير موافق\",\n",
        "    \"🔐\":\"قفل\",\n",
        "    \"👈\":\"لليمين\",\n",
        "    \"™\":\"رمز\",\n",
        "    \"🚶🏽\":\"يتمشي\",\n",
        "    \"😯\":\"متفاجأ\",\n",
        "    \"✊\":\"يد مغلقه\",\n",
        "    \"😻\":\"اعجاب\",\n",
        "    \"🙉\" :\"قرد\",\n",
        "    \"👧\":\"طفله صغيره\",\n",
        "    \"🔴\":\"دائره حمراء\",\n",
        "    \"💪🏽\":\"قوه\",\n",
        "    \"💤\":\"ينام\",\n",
        "    \"👀\":\"ينظر\",\n",
        "    \"✍🏻\":\"يكتب\",\n",
        "    \"❄️\":\"تلج\",\n",
        "    \"💀\":\"رعب\",\n",
        "    \"😤\":\"وجه عابس\",\n",
        "    \"🖋\":\"قلم\",\n",
        "    \"🎩\":\"كاب\",\n",
        "    \"☕️\":\"قهوه\",\n",
        "    \"😹\":\"ضحك\",\n",
        "    \"💓\":\"حب\",\n",
        "    \"☄️\":\"نار\",\n",
        "    \"👻\":\"رعب\",\n",
        "    \"✋\": \"يد\",\n",
        "    \"🌱\": \"نبتة\",\n",
        "\n",
        "    # Emoticons\n",
        "    \":)\" : \"يبتسم\",\n",
        "    \"(:\" : \"يبتسم\",\n",
        "    \":(\" : \"حزين\",\n",
        "    \"xD\" : \"يضحك\",\n",
        "    \":=(\": \"يبكي\",\n",
        "    \":'(\": \"حزن\",\n",
        "    \":'‑(\": \"حزن\",\n",
        "    \"XD\" : \"يضحك\",\n",
        "    \":D\" : \"يبتسم\",\n",
        "    \"♬\" : \"موسيقي\",\n",
        "    \"♡\" : \"حب\",\n",
        "    \"☻\"  : \"يبتسم\",\n",
        "}\n",
        "\n",
        "def replace_emojis(text):\n",
        "    pattern = re.compile('|'.join(re.escape(key) for key in emojis.keys()))\n",
        "    replaced_text = pattern.sub(lambda match: emojis[match.group(0)] + ' ', text)\n",
        "    return emoji.replace_emoji(replaced_text, '')\n",
        "\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: replace_emojis(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ir8Glj9EEsf"
      },
      "source": [
        "## Removing mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXgrvqiKEEsg"
      },
      "outputs": [],
      "source": [
        "pattern = r'@[\\w]+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AA7faVLeEEsh"
      },
      "source": [
        "## Removing links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm75bZvGEEsh"
      },
      "outputs": [],
      "source": [
        "pattern = r'https?://\\S+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMg71jAEEsn"
      },
      "source": [
        "## Remove foriegn words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1EjRxMEEsn"
      },
      "source": [
        "The text includes english, japanese and words for other languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HraPS5_SEEso"
      },
      "outputs": [],
      "source": [
        "pattern = r'[a-zA-Z]+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_r3f6p9DEEsp"
      },
      "source": [
        "## Remove punctuations & special chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2gtB69BEEsp"
      },
      "outputs": [],
      "source": [
        "pattern = r'[^\\w\\s\\u0600-\\u06FF]+|_|ﷺ|۩|⓵|؟|؛|۞|ﷻ|،| ٰ'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-BEPhEhYEEsq"
      },
      "source": [
        "## Remove consecutive characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjEAl55ZEEsr"
      },
      "outputs": [],
      "source": [
        "pattern = r'(.)\\1+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, r'\\1', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove tatweel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_tatweel(document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MZctqr8qEEsr"
      },
      "source": [
        "## Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUfiKGlxEEst"
      },
      "outputs": [],
      "source": [
        "pattern = r'\\d+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove extra whitespaces\n",
        "In this step we get rid of extra whitespaces as well as new lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pattern = r'\\s+|\\n+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove harakat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_tashkeel(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove diactrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_diacritics(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize hamza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pattern = r\"أ|إ|آ\"\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, 'ا', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sy4GggUEEsw"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nf_D_MeEEsx"
      },
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.tokenize(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove long & short words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: [word for word in document if len(word) < 9 and len(word) > 1])\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pQOiy36-EEsy"
      },
      "source": [
        "## Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFgkKc6GEEsy"
      },
      "outputs": [],
      "source": [
        "ar_stemmer = stemmer(\"arabic\")\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda doc: [ar_stemmer.stemWord(token) for token in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arabic_stopwords = stopwords.words('arabic')\n",
        "arabic_stopwords.extend(stp.stopwords_list())\n",
        "stop_words = {ar_stemmer.stemWord(entry) for entry in arabic_stopwords}\n",
        "with open(\"arabic_stopwords.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    for word in file:\n",
        "        stop_words.add(ar_stemmer.stemWord(word.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "open(\"./models/stopwords.pkl\", \"wb\").write(pickle.dumps(stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_stopwords(document: str) -> str:\n",
        "    words = set(document.split(\" \"))\n",
        "    return \" \".join(list(words - stop_words))\n",
        "\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: \" \".join([token for token in document if token not in stop_words]))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw0Z0MsXEEsz"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"preprocessed_data.csv\"):\n",
        "    # remove empty entries\n",
        "    # data.replace('', pd.NA, inplace=True)  # Replace empty strings with NA\n",
        "    # data.dropna(inplace=True)  # Drop rows with NA values\n",
        "    data.to_csv(\"preprocessed_data.csv\") # inspect the resulting file to validate the preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l4AEbbxhEEsz"
      },
      "source": [
        "# Text representation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appraisal analysis\n",
        "Bow + G:AO\n",
        "\n",
        "G:AO Appraisal Group by Attitude & Orientation — Total\n",
        "frequency of appraisal groups with each possible combination of Attitude and Orientation, normalized by total number of appraisal groups in the text."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openpyxl\n",
        "\n",
        "def excel_to_dict(file_path):\n",
        "    workbook = openpyxl.load_workbook(file_path)\n",
        "    data_dict = {}\n",
        "    \n",
        "    for sheet_name in workbook.sheetnames:\n",
        "        sheet = workbook[sheet_name]\n",
        "        values = [cell.value for cell in sheet['A'] if cell.value is not None]\n",
        "        data_dict[sheet_name] = values\n",
        "    \n",
        "    return data_dict\n",
        "\n",
        "# Example usage:\n",
        "file_path = 'example.xlsx'  # Replace 'example.xlsx' with your file path\n",
        "lexicon = excel_to_dict(\"./Arabic seed terms.xlsx\")\n",
        "print(lexicon)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'affect_happ_neg': ['حزين ', 'قانط', 'جزع', 'محطم ', 'ييب ', 'محبط', 'مولم', 'منقبض', 'مثير للشفق', 'مهموم', 'مكتيب', 'مكدر', 'ممل', 'بايس', 'تعيس', 'محز', 'قاتم', 'متج', 'منكسر الخاطر', 'منعزل', 'متد', 'مغتم', 'كاء', 'باك', 'دامع'], 'affect_happ_pos': ['مرح', 'نشط', 'مبتهج'], 'affect_inc_neg': ['حذر', 'خايف', 'مفزوع'], 'affect_satis_neg': ['سطح', 'مبتذل', 'ممل', 'منزعج', 'غاضب', 'مغيظ', 'متضايق', 'ساخط', 'سييم'], 'affect_satis_pos': ['معن', 'منشغل ', 'منهم', 'راض', 'مسرور', 'معجب ', 'سعيد', 'منبهر', 'مفتون', 'مثير', 'مثير للاعجاب'], 'affect_sec_neg': ['مرتب', 'قلق ', 'شاذ', 'مفاجي ', 'مندهش', 'مشد'], 'affect_sec_pos': ['واثق', 'موكد', 'مريح', 'واثق من نفس', 'موضع ثق'], 'apprec_comp_balance_neg': ['مختل', 'متعارض', 'متقطع', 'متفا', 'معيب', 'متناقض', 'وضو', 'مشو', 'بشع', 'محرف'], 'apprec_comp_balance_pos': ['متواز', 'متناغم', 'موحد', 'متماثل', 'متناسب', 'ملايم', 'محترم', 'منطقي ', 'متناسق', 'رشيق', 'مرتب'], 'apprec_comp_complex_neg': ['معقد', 'مفرط', 'يزنط', 'غامض', 'مبهم', 'عكر ', 'عاد', 'اعتياد', 'مبسط'], 'apprec_comp_complex_pos': ['سيط', 'صاف', 'انيق ', 'شفاف', 'واضح', 'دقيق ', 'مركب', 'غن', 'مفصل'], 'apprec_reaction_impact_neg': ['باه', 'ممل', 'مضجر ', 'جاف', 'غامض', 'سقيم', 'سطح', 'متوقع', 'رتيب', 'خاف', 'تاف'], 'apprec_reaction_impact_pos': ['افت للنظر', 'خلاب', 'جذاب ', 'فاتن', 'مثير', 'موثر', 'نشيط', 'مذهل', 'حاد', 'رايع', 'بارز', 'حساس'], 'apprec_reaction_quality_neg': ['سيء', 'كر', 'مقرف ', 'عاد', 'قبيح', 'شع ', 'غيض', 'غاضب', 'مشميز'], 'apprec_reaction_quality_pos': ['مقبول', 'رفيع', 'جيد', 'محبوب', 'جميل', 'رايع', 'جذاب', 'فاتن', 'محتف'], 'apprec_valuation_neg': ['ضحل', 'مختزل', 'تاف', 'ثانو', 'تقليد', 'رك', 'محافظ', 'متاخر', 'قديم', 'شايع', 'مبتذل', 'عاد', 'زايف', 'كاذب', 'مغر', 'رخيص', 'مزيف', 'غال', 'عاجز ', 'عقيم', 'باطل'], 'apprec_valuation_pos': ['نافذ', 'عميق', 'راسخ', 'مبتكر', 'اصيل', 'خلاق', 'مناسب', 'منتظر', 'موشر', 'مميز', 'استثناء', 'ريد', 'اصل', 'حقيقي ', 'نفيس', 'ثمين', 'مهم', 'ملايم', 'مفيد', 'عال'], 'judg_esteem_cap_neg': ['متسامح', 'ضعيف', 'جبان', 'فاسد', 'مريض', 'اعرج', 'فج', 'صب', 'عاجز', 'ممل', 'كييب', 'رهيب', 'طيء', 'غب', 'ثقيل ', 'ابل', 'عصاب', 'مختل', 'ساذج', 'هاو', 'احمق', 'ام', 'خشن', 'جاهل ', 'عاجز', 'ناقص', 'مخفق', 'عقيم'], 'judg_esteem_cap_pos': ['قوي ', 'حيو', 'متين ', 'سليم', 'صح', 'صالح ', 'بالغ', 'ناضج', 'مجرب ', 'بارع', 'هزل', 'طريف ', 'ثاقب', 'ذك', 'موهوب', 'متواز', 'واثق', 'عاقل', 'معقول', 'خبير', 'ماهر', 'عارف', 'مهذب', 'متعلم', 'كفء', 'متمكن ', 'ناجح', 'منتج'], 'judg_esteem_norm_neg': ['مشووم', 'بايس', 'منحوس  ', 'غريب', 'عجيب', 'متقلب', 'تاء', 'غامض ', 'محافظ', 'قديم', 'رجع', 'مبهم', 'فاشل'], 'judg_esteem_norm_pos': ['محظوظ', 'موفق', 'رايع', 'عاد', 'طبيع', 'مالوف ', 'هادء', 'مستقر', 'متوقع ', 'راه', 'عصر', 'طليع', 'مشهور', 'مغمور'], 'judg_esteem_ten_neg': ['خجول', 'جبان', 'خواف ', 'مندفع', 'برم', 'متهور', 'متسرع', 'متقلب', 'طايش ', 'ضعيف', 'غافل', 'جزع  ', 'خاين ', 'مخادع ', 'غادر', 'متغير', 'عنيد', 'متع', 'متعمد'], 'judg_esteem_ten_pos': ['مقدام', 'شجاع', 'طولي  ', 'حذر', 'محترس', 'صابر', 'يقظ', 'كامل', 'دقيق', 'نشيط', 'مثابر', 'حازم', 'موثوق', 'معتمد ', 'مخلص', 'وف', 'ثاب', 'مرن', 'متكيف ', 'متع'], 'judg_sanction_prop_neg': ['سيء', 'سافل', 'شرير ', 'مرتش', 'ظالم', 'جاير ', 'قاس', 'خسيس', 'شرس', 'تاف', 'متكبر', 'متغطرس', 'وقح', 'فظ', 'محتقر', 'انا', 'جشع', 'خيل'], 'judg_sanction_prop_pos': ['صالح', 'معنو', 'اخلاقي ', 'ملتزم بالق', 'عادل', 'منصف', 'مرهف', 'لطيف', 'حريص  ', 'سيط', 'متواضع', 'ديع  ', 'مهذب', 'محترم', 'موقر', 'ايثار', 'سخ', 'محس'], 'judg_sanction_ver_neg': ['غشاش', 'مخادع', 'ذاب ', 'مضلل', 'متلاعب', 'مراوغ', 'كليل', 'ثرثار'], 'judg_sanction_ver_pos': ['صادق', 'نز', 'موثوق ', 'صريح', 'واضح', 'مباشر  ', 'محترس', 'لبق'], 'verb_affect_happ_neg': ['كره', 'ابغض', 'مقت'], 'verb_affect_happ_pos': ['مال', 'حب', 'عشق', 'ولع'], 'verb_affect_inc_pos': ['افتقد', 'اشتاق', 'تاق']}\n"
          ]
        }
      ],
      "source": [
        "ar_stemmer = stemmer(\"arabic\")\n",
        "for key, val in lexicon.items():\n",
        "    for idx, val in enumerate(val):\n",
        "        preprocessed_val = araby.strip_tashkeel(val)\n",
        "        preprocessed_val = araby.strip_diacritics(preprocessed_val)\n",
        "        preprocessed_val = re.sub(r\"أ|إ|آ\", 'ا', preprocessed_val)\n",
        "        preprocessed_val = ar_stemmer.stemWord(preprocessed_val)\n",
        "        lexicon[key][idx] = preprocessed_val\n",
        "print(lexicon)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appraisal features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construct a word to appraisal group mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_to_appraisal_grp = {}\n",
        "for key, val in lexicon.items():\n",
        "    for values in val:\n",
        "        word_to_appraisal_grp[values] = key\n",
        "word_to_appraisal_grp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "appraisal_grp_to_idx = {}\n",
        "idx = 0\n",
        "for appraisal_grp in list(lexicon.keys()):\n",
        "    appraisal_grp_to_idx[appraisal_grp] = idx\n",
        "    idx += 1\n",
        "appraisal_grp_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def appraisal_features(document: str) -> List[float]:\n",
        "    res = np.zeros(len(lexicon))\n",
        "    tokens = [word for word in document.split(\" \")]\n",
        "    count_appraisal_grps = 0\n",
        "    for token in tokens:\n",
        "        if token in word_to_appraisal_grp:\n",
        "            res[ appraisal_grp_to_idx[word_to_appraisal_grp[token]] ] += 1\n",
        "            count_appraisal_grps += 1\n",
        "    # normalize features by the count of appraisal groups if the count != 0\n",
        "    res = res / count_appraisal_grps if count_appraisal_grps != 0 else res\n",
        "    return res\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(24742,)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify = data[\"class\"])\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train_BOW = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_BOW = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "X_train_BOW[0].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "construct training and testing appraisal feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(23642, 30)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_appraisal = np.array(\n",
        "    [appraisal_features(document) for document in X_train]\n",
        ")\n",
        "\n",
        "X_train_appraisal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5911, 30)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test_appraisal = np.array(\n",
        "    [appraisal_features(document) for document in X_test]\n",
        ")\n",
        "\n",
        "X_test_appraisal.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Features' union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(23642, 24772)\n",
            "(5911, 24772)\n"
          ]
        }
      ],
      "source": [
        "X_train = np.hstack((X_train_BOW, X_train_appraisal))\n",
        "X_test  = np.hstack((X_test_BOW, X_test_appraisal))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance evaluation\n",
        "## Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 58.53%\n",
            "Precesion : 55.61%\n",
            "Recall : 75.55%\n",
            "F1 score : 64.07%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/APPRAISAL/NB.pkl\"):\n",
        "    # read model from disk\n",
        "    model = open(\"./models/ASTC/APPRAISAL/NB.pkl\", 'rb').read()\n",
        "    model: GaussianNB = pickle.loads(model)\n",
        "else:\n",
        "    model = GaussianNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    # write model to disk\n",
        "    mdl_bytes = pickle.dumps(model)\n",
        "    open(\"./models/ASTC/APPRAISAL/NB.pkl\", 'wb').write(mdl_bytes)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_N8hCmIEEtK",
        "outputId": "7fd72e2e-c809-445a-851d-035c5e8381eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 87.50%\n",
            "Precesion : 85.21%\n",
            "Recall : 90.08%\n",
            "F1 score : 87.58%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/APPRAISAL/LR.pkl\"):\n",
        "    # read model from disk\n",
        "    model = open(\"./models/ASTC/APPRAISAL/LR.pkl\", 'rb').read()\n",
        "    model: LogisticRegression = pickle.loads(model)\n",
        "else:\n",
        "    model = LogisticRegression(random_state = SEED, max_iter = 1500)\n",
        "    model.fit(X_train, y_train)\n",
        "    # write model to disk\n",
        "    mdl_bytes = pickle.dumps(model)\n",
        "    open(\"./models/ASTC/APPRAISAL/LR.pkl\", 'wb').write(mdl_bytes)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM\n",
        "\n",
        "We were unable to train this model on our machines using the initial dataset, due to the **curse of dimensionality**, so we added dimensioanlity reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 86.84%\n",
            "Precesion : 82.48%\n",
            "Recall : 92.81%\n",
            "F1 score : 87.34%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/APPRAISAL/SVM_Pipeline.pkl\"):\n",
        "    # read pipeline from disk\n",
        "    pipeline = open(\"./models/ASTC/APPRAISAL/SVM_Pipeline.pkl\", 'rb').read()\n",
        "    pipeline = pickle.loads(pipeline)\n",
        "else:\n",
        "    steps = [\n",
        "        ('pca', PCA(n_components = 150, random_state = SEED)),\n",
        "        ('svm', SVC(random_state = SEED))\n",
        "    ]\n",
        "    pipeline = Pipeline(steps = steps)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    # write pipeline to disk\n",
        "    pipeline_bytes = pickle.dumps(pipeline)\n",
        "    open(\"./models/ASTC/APPRAISAL/SVM_Pipeline.pkl\", 'wb').write(pipeline_bytes)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label = \"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 86.84%\n",
            "Precesion : 82.48%\n",
            "Recall : 92.81%\n",
            "F1 score : 87.34%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/APPRAISAL/RF.pkl\"):\n",
        "    # read model from disk\n",
        "    model = open(\"./models/ASTC/APPRAISAL/RF.pkl\", 'rb').read()\n",
        "    model = pickle.loads(model)\n",
        "else:\n",
        "    model = RandomForestClassifier(random_state = SEED)\n",
        "    model.fit(X_train, y_train)\n",
        "    # write model to disk\n",
        "    mdl_bytes = pickle.dumps(model)\n",
        "    open(\"./models/ASTC/APPRAISAL/RF.pkl\", 'wb').write(mdl_bytes)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
