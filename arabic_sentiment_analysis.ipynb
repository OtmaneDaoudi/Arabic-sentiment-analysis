{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OtmaneDaoudi/Arabic-sentiment-analysis/blob/main/arabic_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YCMH2OjBGA8u"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BqPG4jsGF8m",
        "outputId": "71519abe-6ccb-4059-c163-edb45164bcbe"
      },
      "outputs": [],
      "source": [
        "# !pip install emoji\n",
        "# !pip install Arabic-Stopwords\n",
        "# !pip install seaborn\n",
        "# !pip install matplotlib\n",
        "# !pip install soyclustering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KUh0GecqEErv"
      },
      "source": [
        "# Libs imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQdYY3KPEEsC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "import pickle\n",
        "\n",
        "import arabicstopwords.arabicstopwords as stp\n",
        "import pandas as pd\n",
        "import pyarabic.araby as araby\n",
        "import numpy as np \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "from snowballstemmer import stemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from typing import List\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "SEED = 21"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "MvI6l3cnEEsH",
        "outputId": "840f53a3-1120-4709-a395-10f90977c0a9"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"./datasets/ASTC/data.tsv\", header = 0, sep='\\t', names = [\"class\", \"tweet\"]).sample(frac = 1, random_state = SEED)\n",
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqggWownEEsK",
        "outputId": "e8a398e4-bfd2-44cf-b85f-2ea0ab850aa8"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GIECLLDlEEsM"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NDtOpDzdEEsP"
      },
      "source": [
        "Our preprocessing pipeline contains the following steps:\n",
        "\n",
        "1.  Remove duplicat entries\n",
        "2.  Replacing emojies & emoticons\n",
        "3.  Remove mentions\n",
        "4.  Remove Links\n",
        "5.  Remove whitespaces\n",
        "6.  Remove punctuations & Special chars\n",
        "7.  Remove Consecutive characters\n",
        "8.  Tokenization\n",
        "9.  Remove foreign words\n",
        "10. Remove stop words\n",
        "11. Remove numbers\n",
        "12. Stemming\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zolKP03vEEsT"
      },
      "source": [
        "## Removing duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeMI9vGpEEsW",
        "outputId": "e2fb0124-2a17-4db2-c06f-52d48a0b7000"
      },
      "outputs": [],
      "source": [
        "count = data.duplicated().sum()\n",
        "print(f\"{(count / data.shape[0]) * 100:.1f}% of the data are duplicats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahlc7NfGEEsY"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(inplace = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv54Q2aKEEsZ"
      },
      "source": [
        "## Replacing emojies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Dy9RLbEEsa"
      },
      "outputs": [],
      "source": [
        "emojis = {\n",
        "    \"ğŸ™‚\":\"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"ğŸ˜‚\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸ¤£\" : \"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸ’”\":\"Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†\",\n",
        "    \"ğŸ™‚\":\"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"â¤ï¸\":\"Ø­Ø¨\",\n",
        "    \"ğŸ¥°\":\"Ø­Ø¨\",\n",
        "    \"ğŸ¤\":\"Ø³ÙƒÙˆØª\",\n",
        "    \"ğŸ§¡\":\"Ø­Ø¨\",\n",
        "    \"â¤\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜­\":\"ÙŠØ¨ÙƒÙŠ\",\n",
        "    \"ğŸ¤­\":\"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"ğŸ˜¢\":\"Ø­Ø²Ù†\",\n",
        "    \"ğŸ˜”\":\"Ø­Ø²Ù†\",\n",
        "    \"â™¥\":\"Ø­Ø¨\",\n",
        "    \"ğŸ’œ\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜…\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸ™\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ğŸ’•\":\"Ø­Ø¨\",\n",
        "    \"ğŸ’™\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ğŸ˜Š\":\"Ø³Ø¹Ø§Ø¯Ø©\",\n",
        "    \"ğŸ‘\":\"ÙŠØµÙÙ‚\",\n",
        "    \"ğŸ‘Œ\":\"Ø§Ø­Ø³Ù†Øª\",\n",
        "    \"ğŸ˜´\":\"ÙŠÙ†Ø§Ù…\",\n",
        "    \"ğŸ˜€\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"âœ…\":\"ØµØ­ÙŠØ­\",\n",
        "    \"ğŸ¤ª\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸ¡\" : \"Ø¨ÙŠØª\",\n",
        "    \"ğŸ¤²\" : \"Ø¯Ø¹Ø§Ø¡\",\n",
        "    \"ğŸ’°\" : \"Ù…Ø§Ù„\",\n",
        "    \"ğŸ˜Œ\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ğŸ\":\"Ù‡Ø¯ÙŠØ©\",\n",
        "    \"ğŸŒ¹\":\"ÙˆØ±Ø¯Ø©\",\n",
        "    \"ğŸ¥€\":\"ÙˆØ±Ø¯Ø©\",\n",
        "    \"ğŸ“¿\":\"ÙˆØ±Ø¯Ø©\",\n",
        "    \"âœ\":\"ÙƒØªØ§Ø¨Ø©\",\n",
        "    \"ğŸ™ˆ\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜„\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸ˜\":\"Ù…Ø­Ø§ÙŠØ¯\",\n",
        "    \"âœŒ\":\"Ù…Ù†ØªØµØ±\",\n",
        "    \"âœ¨\":\"Ù†Ø¬Ù…Ù‡\",\n",
        "    \"ğŸ¤”\":\"ØªÙÙƒÙŠØ±\",\n",
        "    \"ğŸ˜\":\"ÙŠØ³ØªÙ‡Ø²Ø¡\",\n",
        "    \"ğŸ˜’\":\"ÙŠØ³ØªÙ‡Ø²Ø¡\",\n",
        "    \"ğŸ™„\":\"Ù…Ù„Ù„\",\n",
        "    \"ğŸ˜•\":\"Ø¹ØµØ¨ÙŠØ©\",\n",
        "    \"ğŸ˜ƒ\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸŒ¸\":\"ÙˆØ±Ø¯Ø©\",\n",
        "    \"ğŸ˜“\":\"Ø­Ø²Ù†\",\n",
        "    \"ğŸ’\":\"Ø­Ø¨\",\n",
        "    \"ğŸ’—\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜‘\":\"Ù…Ù†Ø²Ø¹Ø¬\",\n",
        "    \"ğŸ’­\":\"ØªÙÙƒÙŠØ±\",\n",
        "    \"ğŸ˜\":\"Ø«Ù‚Ø©\",\n",
        "    \"ğŸ’›\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜©\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ğŸ¥º\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ğŸ’ª\":\"Ø¹Ø¶Ù„Ø§Øª\",\n",
        "    \"ğŸ‘\":\"Ù…ÙˆØ§ÙÙ‚\",\n",
        "    \"ğŸ™ğŸ»\":\"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨\",\n",
        "    \"ğŸ˜³\":\"Ù…ØµØ¯ÙˆÙ…\",\n",
        "    \"ğŸ‘ğŸ¼\":\"ØªØµÙÙŠÙ‚\",\n",
        "    \"ğŸ¶\":\"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\",\n",
        "    \"ğŸŒš\":\"ØµÙ…Øª\",\n",
        "    \"ğŸ’š\":\"Ø­Ø¨\",\n",
        "    \"ğŸ™\":\"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨\",\n",
        "    \"ğŸ’˜\":\"Ø­Ø¨\",\n",
        "    \"ğŸƒ\":\"Ø³Ù„Ø§Ù…\",\n",
        "    \"â˜º\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸŠ\":\"ÙŠÙ‡Ù†Ø¦\",\n",
        "    \"ğŸ’¥\":\"Ø¥Ù†ÙØ¬Ø§Ø±\",\n",
        "    \"ğŸ˜\":\"ÙŠØ³Ø®Ø±\",\n",
        "    \"ğŸ’¯\":\"ØªÙ…Ø§Ù…\",\n",
        "    \"ğŸ¸\":\"Ø¶ÙØ¯Ø¹\",\n",
        "    \"ğŸ¤¦â€â™‚ï¸\":\"ØºØ¨ÙŠ\",\n",
        "    \"ğŸ¤©\":\"Ù…Ø¹Ø¬Ø¨\",\n",
        "    \"ğŸ¤¤\":\"Ø¬Ø§Ø¦Ø¹\",\n",
        "    \"ğŸ˜¶\":\"Ù…ØµØ¯ÙˆÙ…\",\n",
        "    \"âœŒï¸\":\"Ù…Ø±Ø­\",\n",
        "    \"âœ‹ğŸ»\":\"ØªÙˆÙ‚Ù\",\n",
        "    \"ğŸ˜‰\":\"ØºÙ…Ø²Ø©\",\n",
        "    \"ğŸŒ·\":\"Ø­Ø¨\",\n",
        "    \"ğŸ™ƒ\":\"Ù…Ø¨ØªØ³Ù…\",\n",
        "    \"ğŸ˜«\":\"Ø­Ø²ÙŠÙ†\",\n",
        "    \"ğŸ˜¨\":\"Ù…ØµØ¯ÙˆÙ…\",\n",
        "    \"ğŸ¼ \":\"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\",\n",
        "    \"ğŸ\":\"Ù…Ø±Ø­\",\n",
        "    \"ğŸ‚\":\"Ù…Ø±Ø­\",\n",
        "    \"ğŸ’Ÿ\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜ª\":\"Ø­Ø²Ù†\",\n",
        "    \"ğŸ˜†\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸ˜£\":\"Ø§Ø³ØªÙŠØ§Ø¡\",\n",
        "    \"â˜ºï¸\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜±\":\"ÙƒØ§Ø±Ø«Ø©\",\n",
        "    \"ğŸ˜\":\"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \"ğŸ˜–\":\"Ø§Ø³ØªÙŠØ§Ø¡\",\n",
        "    \"ğŸƒğŸ¼\":\"ÙŠØ¬Ø±ÙŠ\",\n",
        "    \"ğŸ˜¡\":\"ØºØ¶Ø¨\",\n",
        "    \"ğŸš¶\":\"ÙŠØ³ÙŠØ±\",\n",
        "    \"ğŸ¤•\":\"Ù…Ø±Ø¶\",\n",
        "    \"ğŸ¤®\" : \"ÙŠØªÙ‚ÙŠØ¦\",\n",
        "    \"â›”\": \"Ø­Ø°Ø±\",\n",
        "    \"â€¼ï¸\":\"ØªØ¹Ø¬Ø¨\",\n",
        "    \"ğŸ•Š\":\"Ø·Ø§Ø¦Ø±\",\n",
        "    \"ğŸ‘ŒğŸ»\":\"Ø§Ø­Ø³Ù†Øª\",\n",
        "    \"â£\":\"Ø­Ø¨\",\n",
        "    \"ğŸ™Š\":\"Ù…ØµØ¯ÙˆÙ…\",\n",
        "    \"ğŸ’ƒ\":\"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­\",\n",
        "    \"ğŸ’ƒğŸ¼\":\"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­\",\n",
        "    \"ğŸ˜œ\":\"Ù…Ø±Ø­\",\n",
        "    \"ğŸ‘Š\":\"Ø¶Ø±Ø¨Ø©\",\n",
        "    \"ğŸ˜Ÿ\":\"Ø§Ø³ØªÙŠØ§Ø¡\",\n",
        "    \"ğŸ’–\":\"Ø­Ø¨\",\n",
        "    \"ğŸ˜¥\":\"Ø­Ø²Ù†\",\n",
        "    \"ğŸ»\":\"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\",\n",
        "    \"âœ’\":\"ÙŠÙƒØªØ¨\",\n",
        "    \"ğŸš¶ğŸ»\":\"ÙŠØ³ÙŠØ±\",\n",
        "    \"ğŸ’\":\"Ø§Ù„Ù…Ø§Ø¸\",\n",
        "    \"ğŸ˜·\":\"ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶\",\n",
        "    \"â˜\":\"ÙˆØ§Ø­Ø¯\",\n",
        "    \"ğŸš¬\":\"ØªØ¯Ø®ÙŠÙ†\",\n",
        "    \"ğŸ’\" : \"ÙˆØ±Ø¯\",\n",
        "    \"ğŸŒ»\" : \"ÙˆØ±Ø¯\",\n",
        "    \"ğŸŒ\" : \"Ø´Ù…Ø³\",\n",
        "    \"ğŸ‘†\" : \"Ø§Ù„Ø§ÙˆÙ„\",\n",
        "    \"âš ï¸\" :\"ØªØ­Ø°ÙŠØ±\",\n",
        "    \"ğŸ¤—\" : \"Ø§Ø­ØªÙˆØ§Ø¡\",\n",
        "    \"âœ–ï¸\": \"ØºÙ„Ø·\",\n",
        "    \"ğŸ“\"  : \"Ù…ÙƒØ§Ù†\",\n",
        "    \"ğŸ‘¸\" : \"Ù…Ù„ÙƒÙ‡\",\n",
        "    \"ğŸ‘‘\" : \"ØªØ§Ø¬\",\n",
        "    \"âœ”ï¸\" : \"ØµØ­\",\n",
        "    \"ğŸ’Œ\": \"Ù‚Ù„Ø¨\",\n",
        "    \"ğŸ˜²\" : \"Ù…Ù†Ø¯Ù‡Ø´\",\n",
        "    \"ğŸ’¦\": \"Ù…Ø§Ø¡\",\n",
        "    \"ğŸš«\" : \"Ø®Ø·Ø§\",\n",
        "    \"ğŸ‘ğŸ»\" : \"Ø¨Ø±Ø§ÙÙˆ\",\n",
        "    \"ğŸŠ\" :\"ÙŠØ³Ø¨Ø­\",\n",
        "    \"ğŸ‘ğŸ»\": \"ØªÙ…Ø§Ù…\",\n",
        "    \"â­•ï¸\" :\"Ø¯Ø§Ø¦Ø±Ù‡ ÙƒØ¨ÙŠØ±Ù‡\",\n",
        "    \"ğŸ·\" : \"Ø³Ø§ÙƒØ³ÙÙˆÙ†\",\n",
        "    \"ğŸ‘‹\": \"ØªÙ„ÙˆÙŠØ­ Ø¨Ø§Ù„ÙŠØ¯\",\n",
        "    \"âœŒğŸ¼\": \"Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±\",\n",
        "    \"ğŸŒ\":\"Ù…Ø¨ØªØ³Ù…\",\n",
        "    \"â¿\"  : \"Ø¹Ù‚Ø¯Ù‡ Ù…Ø²Ø¯ÙˆØ¬Ù‡\",\n",
        "    \"ğŸ’ªğŸ¼\" : \"Ù‚ÙˆÙŠ\",\n",
        "    \"ğŸ“©\":  \"ØªÙˆØ§ØµÙ„ Ù…Ø¹ÙŠ\",\n",
        "    \"â˜•ï¸\": \"Ù‚Ù‡ÙˆÙ‡\",\n",
        "    \"ğŸ˜§\" : \"Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©\",\n",
        "    \"ğŸ—¨\": \"Ø±Ø³Ø§Ù„Ø©\",\n",
        "    \"â—ï¸\" :\"ØªØ¹Ø¬Ø¨\",\n",
        "    \"ğŸ™†ğŸ»\": \"Ø§Ø´Ø§Ø±Ù‡ Ù…ÙˆØ§ÙÙ‚Ù‡\",\n",
        "    \"ğŸ‘¯\" :\"Ø§Ø®ÙˆØ§Øª\",\n",
        "    \"Â©\" :  \"Ø±Ù…Ø²\",\n",
        "    \"ğŸ‘µğŸ½\" :\"Ø³ÙŠØ¯Ù‡ Ø¹Ø¬ÙˆØ²Ù‡\",\n",
        "    \"ğŸ£\": \"ÙƒØªÙƒÙˆØª\",\n",
        "    \"ğŸ™Œ\": \"ØªØ´Ø¬ÙŠØ¹\",\n",
        "    \"ğŸ™‡\": \"Ø´Ø®Øµ ÙŠÙ†Ø­Ù†ÙŠ\",\n",
        "    \"ğŸ‘ğŸ½\":\"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡\",\n",
        "    \"ğŸ‘ŒğŸ½\": \"Ø¨Ø§Ù„Ø¸Ø¨Ø·\",\n",
        "    \"â‰ï¸\" : \"Ø§Ø³ØªÙ†ÙƒØ§Ø±\",\n",
        "    \"âš½ï¸\": \"ÙƒÙˆØ±Ù‡\",\n",
        "    \"ğŸ•¶\" :\"Ø­Ø¨\",\n",
        "    \"ğŸˆ\" :\"Ø¨Ø§Ù„ÙˆÙ†\",\n",
        "    \"ğŸ€\":    \"ÙˆØ±Ø¯Ù‡\",\n",
        "    \"ğŸ’µ\":  \"ÙÙ„ÙˆØ³\",\n",
        "    \"ğŸ˜‹\":  \"Ø¬Ø§Ø¦Ø¹\",\n",
        "    \"ğŸ˜›\":  \"ÙŠØºÙŠØ¸\",\n",
        "    \"ğŸ˜ \":  \"ØºØ§Ø¶Ø¨\",\n",
        "    \"âœğŸ»\":  \"ÙŠÙƒØªØ¨\",\n",
        "    \"ğŸŒ¾\":  \"Ø§Ø±Ø²\",\n",
        "    \"ğŸ‘£\":  \"Ø§Ø«Ø± Ù‚Ø¯Ù…ÙŠÙ†\",\n",
        "    \"âŒ\":\"Ø±ÙØ¶\",\n",
        "    \"ğŸŸ\":\"Ø·Ø¹Ø§Ù…\",\n",
        "    \"ğŸ‘¬\":\"ØµØ¯Ø§Ù‚Ø©\",\n",
        "    \"ğŸ°\":\"Ø§Ø±Ù†Ø¨\",\n",
        "    \"ğŸ¦‹\" : \"ÙØ±Ø§Ø´Ø©\",\n",
        "    \"â˜‚\":\"Ù…Ø·Ø±\",\n",
        "    \"âšœ\":\"Ù…Ù…Ù„ÙƒØ© ÙØ±Ù†Ø³Ø§\",\n",
        "    \"ğŸ‘\":\"Ø®Ø±ÙˆÙ\",\n",
        "    \"ğŸ—£\":\"ØµÙˆØª Ù…Ø±ØªÙØ¹\",\n",
        "    \"ğŸ‘ŒğŸ¼\":\"Ø§Ø­Ø³Ù†Øª\",\n",
        "    \"â˜˜\":\"Ù…Ø±Ø­\",\n",
        "    \"ğŸ˜®\":\"ØµØ¯Ù…Ø©\",\n",
        "    \"ğŸ˜¦\":\"Ù‚Ù„Ù‚\",\n",
        "    \"â­•\":\"Ø§Ù„Ø­Ù‚\",\n",
        "    \"âœï¸\":\"Ù‚Ù„Ù…\",\n",
        "    \"â„¹\":\"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª\",\n",
        "    \"ğŸ™ğŸ»\":\"Ø±ÙØ¶\",\n",
        "    \"âšªï¸\":\"Ù†Ø¶Ø§Ø±Ø© Ù†Ù‚Ø§Ø¡\",\n",
        "    \"ğŸ¤\":\"Ø­Ø²Ù†\",\n",
        "    \"ğŸ’«\":\"Ù…Ø±Ø­\",\n",
        "    \"ğŸ’\":\"Ø­Ø¨\",\n",
        "    \"ğŸ”\":\"Ø·Ø¹Ø§Ù…\",\n",
        "    \"â¤ï¸\":\"Ø­Ø¨\",\n",
        "    \"âœˆï¸\":\"Ø³ÙØ±\",\n",
        "    \"ğŸƒğŸ»â€â™€ï¸\":\"ÙŠØ³ÙŠØ±\",\n",
        "    \"ğŸ³\":\"Ø°ÙƒØ±\",\n",
        "    \"ğŸ¤\":\"Ù…Ø§ÙŠÙƒ ØºÙ†Ø§Ø¡\",\n",
        "    \"ğŸ¾\":\"ÙƒØ±Ù‡\",\n",
        "    \"ğŸ”\":\"Ø¯Ø¬Ø§Ø¬Ø©\",\n",
        "    \"ğŸ™‹\":\"Ø³Ø¤Ø§Ù„\",\n",
        "    \"ğŸ“®\":\"Ø¨Ø­Ø±\",\n",
        "    \"ğŸ’‰\":\"Ø¯ÙˆØ§Ø¡\",\n",
        "    \"ğŸ™ğŸ¼\":\"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨\",\n",
        "    \"ğŸ’‚ğŸ¿ \":\"Ø­Ø§Ø±Ø³\",\n",
        "    \"ğŸ¬\":\"Ø³ÙŠÙ†Ù…Ø§\",\n",
        "    \"â™¦ï¸\":\"Ù…Ø±Ø­\",\n",
        "    \"ğŸ’¡\":\"Ù‚ÙƒØ±Ø©\",\n",
        "    \"â€¼\":\"ØªØ¹Ø¬Ø¨\",\n",
        "    \"ğŸ‘¼\":\"Ø·ÙÙ„\",\n",
        "    \"ğŸ”‘\":\"Ù…ÙØªØ§Ø­\",\n",
        "    \"â™¥ï¸\":\"Ø­Ø¨\",\n",
        "    \"ğŸŒ²\" : \"Ø´Ø¬Ø±Ø©\",\n",
        "    \"ğŸŒ³\" : \"Ø´Ø¬Ø±Ø©\",\n",
        "    \"ğŸš©\" : \"Ø­Ø°Ø±\",\n",
        "    \"ğŸš¨\" : \"Ø­Ø°Ø±\",\n",
        "    \"ğŸ›‘\" : \"Ø­Ø°Ø±\",\n",
        "    \"ğŸ•‹\":\"ÙƒØ¹Ø¨Ø©\",\n",
        "    \"ğŸ“\":\"Ø¯Ø¬Ø§Ø¬Ø©\",\n",
        "    \"ğŸ’©\":\"Ù…Ø¹ØªØ±Ø¶\",\n",
        "    \"ğŸ‘½\":\"ÙØ¶Ø§Ø¦ÙŠ\",\n",
        "    \"â˜”ï¸\":\"Ù…Ø·Ø±\",\n",
        "    \"ğŸ·\":\"Ø¹ØµÙŠØ±\",\n",
        "    \"ğŸŒŸ\":\"Ù†Ø¬Ù…Ø©\",\n",
        "    \"â˜ï¸\":\"Ø³Ø­Ø¨\",\n",
        "    \"ğŸ‘ƒ\":\"Ù…Ø¹ØªØ±Ø¶\",\n",
        "    \"ğŸŒº\":\"Ù…Ø±Ø­\",\n",
        "    \"ğŸ”ª\":\"Ø³ÙƒÙŠÙ†Ø©\",\n",
        "    \"â™¨\":\"Ø³Ø®ÙˆÙ†ÙŠØ©\",\n",
        "    \"ğŸ‘ŠğŸ¼\":\"Ø¶Ø±Ø¨\",\n",
        "    \"âœ\":\"Ù‚Ù„Ù…\",\n",
        "    \"ğŸš¶ğŸ¾â€â™€ï¸\":\"ÙŠØ³ÙŠØ±\",\n",
        "    \"ğŸ‘Š\":\"Ø¶Ø±Ø¨Ø©\",\n",
        "    \"â—¾ï¸\":\"ÙˆÙ‚Ù\",\n",
        "    \"ğŸ˜š\":\"Ø­Ø¨\",\n",
        "    \"ğŸ”¸\":\"Ù…Ø±Ø­\",\n",
        "    \"ğŸ‘ğŸ»\":\"Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ\",\n",
        "    \"ğŸ‘ŠğŸ½\":\"Ø¶Ø±Ø¨Ø©\",\n",
        "    \"ğŸ˜™\":\"Ø­Ø¨\",\n",
        "    \"ğŸ¥\":\"ØªØµÙˆÙŠØ±\",\n",
        "    \"ğŸ‘‰\":\"Ø¬Ø°Ø¨ Ø§Ù†ØªØ¨Ø§Ù‡\",\n",
        "    \"ğŸ‘ğŸ½\":\"ÙŠØµÙÙ‚\",\n",
        "    \"ğŸ’ªğŸ»\":\"Ø¹Ø¶Ù„Ø§Øª\",\n",
        "    \"ğŸ´\":\"Ø§Ø³ÙˆØ¯\",\n",
        "    \"ğŸ”¥\":\"Ø­Ø±ÙŠÙ‚\",\n",
        "    \"ğŸ˜¬\":\"Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©\",\n",
        "    \"ğŸ‘ŠğŸ¿\":\"ÙŠØ¶Ø±Ø¨\",\n",
        "    \"ğŸ“š\" : \"ÙƒØªØ¨\",\n",
        "    \"ğŸ“Œ\" : \"Ø¹Ù„Ù‚\",\n",
        "    \"ğŸŒ¿\":\"ÙˆØ±Ù‚Ù‡ Ø´Ø¬Ø±Ù‡\",\n",
        "    \"âœ‹ğŸ¼\":\"ÙƒÙ Ø§ÙŠØ¯\",\n",
        "    \"ğŸ‘\":\"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡\",\n",
        "    \"â˜ ï¸\":\"ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨\",\n",
        "    \"ğŸ‰\":\"ÙŠÙ‡Ù†Ø¦\",\n",
        "    \"ğŸ”•\" :\"ØµØ§Ù…Øª\",\n",
        "    \"ğŸ˜¿\":\"ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†\",\n",
        "    \"â˜¹ï¸\":\"ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³\",\n",
        "    \"ğŸ˜˜\" :\"Ø­Ø¨\",\n",
        "    \"ğŸ˜°\" :\"Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†\",\n",
        "    \"ğŸŒ¼\":\"ÙˆØ±Ø¯Ù‡\",\n",
        "    \"ğŸ’‹\": \"Ø¨ÙˆØ³Ù‡\",\n",
        "    \"ğŸ‘‡\":\"Ù„Ø§Ø³ÙÙ„\",\n",
        "    \"â£ï¸\":\"Ø­Ø¨\",\n",
        "    \"ğŸ§\":\"Ø³Ù…Ø§Ø¹Ø§Øª\",\n",
        "    \"ğŸ“\":\"ÙŠÙƒØªØ¨\",\n",
        "    \"ğŸ˜‡\":\"Ø¯Ø§ÙŠØ®\",\n",
        "    \"ğŸ˜ˆ\":\"Ø±Ø¹Ø¨\",\n",
        "    \"ğŸƒ\":\"ÙŠØ¬Ø±ÙŠ\",\n",
        "    \"âœŒğŸ»\":\"Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±\",\n",
        "    \"ğŸ”«\":\"ÙŠØ¶Ø±Ø¨\",\n",
        "    \"â—ï¸\":\"ØªØ¹Ø¬Ø¨\",\n",
        "    \"ğŸ‘\":\"ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚\",\n",
        "    \"ğŸ”\":\"Ù‚ÙÙ„\",\n",
        "    \"ğŸ‘ˆ\":\"Ù„Ù„ÙŠÙ…ÙŠÙ†\",\n",
        "    \"â„¢\":\"Ø±Ù…Ø²\",\n",
        "    \"ğŸš¶ğŸ½\":\"ÙŠØªÙ…Ø´ÙŠ\",\n",
        "    \"ğŸ˜¯\":\"Ù…ØªÙØ§Ø¬Ø£\",\n",
        "    \"âœŠ\":\"ÙŠØ¯ Ù…ØºÙ„Ù‚Ù‡\",\n",
        "    \"ğŸ˜»\":\"Ø§Ø¹Ø¬Ø§Ø¨\",\n",
        "    \"ğŸ™‰\" :\"Ù‚Ø±Ø¯\",\n",
        "    \"ğŸ‘§\":\"Ø·ÙÙ„Ù‡ ØµØºÙŠØ±Ù‡\",\n",
        "    \"ğŸ”´\":\"Ø¯Ø§Ø¦Ø±Ù‡ Ø­Ù…Ø±Ø§Ø¡\",\n",
        "    \"ğŸ’ªğŸ½\":\"Ù‚ÙˆÙ‡\",\n",
        "    \"ğŸ’¤\":\"ÙŠÙ†Ø§Ù…\",\n",
        "    \"ğŸ‘€\":\"ÙŠÙ†Ø¸Ø±\",\n",
        "    \"âœğŸ»\":\"ÙŠÙƒØªØ¨\",\n",
        "    \"â„ï¸\":\"ØªÙ„Ø¬\",\n",
        "    \"ğŸ’€\":\"Ø±Ø¹Ø¨\",\n",
        "    \"ğŸ˜¤\":\"ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³\",\n",
        "    \"ğŸ–‹\":\"Ù‚Ù„Ù…\",\n",
        "    \"ğŸ©\":\"ÙƒØ§Ø¨\",\n",
        "    \"â˜•ï¸\":\"Ù‚Ù‡ÙˆÙ‡\",\n",
        "    \"ğŸ˜¹\":\"Ø¶Ø­Ùƒ\",\n",
        "    \"ğŸ’“\":\"Ø­Ø¨\",\n",
        "    \"â˜„ï¸\":\"Ù†Ø§Ø±\",\n",
        "    \"ğŸ‘»\":\"Ø±Ø¹Ø¨\",\n",
        "    \"âœ‹\": \"ÙŠØ¯\",\n",
        "    \"ğŸŒ±\": \"Ù†Ø¨ØªØ©\",\n",
        "\n",
        "    # Emoticons\n",
        "    \":)\" : \"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"(:\" : \"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \":(\" : \"Ø­Ø²ÙŠÙ†\",\n",
        "    \"xD\" : \"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \":=(\": \"ÙŠØ¨ÙƒÙŠ\",\n",
        "    \":'(\": \"Ø­Ø²Ù†\",\n",
        "    \":'â€‘(\": \"Ø­Ø²Ù†\",\n",
        "    \"XD\" : \"ÙŠØ¶Ø­Ùƒ\",\n",
        "    \":D\" : \"ÙŠØ¨ØªØ³Ù…\",\n",
        "    \"â™¬\" : \"Ù…ÙˆØ³ÙŠÙ‚ÙŠ\",\n",
        "    \"â™¡\" : \"Ø­Ø¨\",\n",
        "    \"â˜»\"  : \"ÙŠØ¨ØªØ³Ù…\",\n",
        "}\n",
        "\n",
        "def replace_emojis(text):\n",
        "    pattern = re.compile('|'.join(re.escape(key) for key in emojis.keys()))\n",
        "    replaced_text = pattern.sub(lambda match: emojis[match.group(0)] + ' ', text)\n",
        "    return emoji.replace_emoji(replaced_text, '')\n",
        "\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: replace_emojis(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ir8Glj9EEsf"
      },
      "source": [
        "## Removing mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXgrvqiKEEsg"
      },
      "outputs": [],
      "source": [
        "pattern = r'@[\\w]+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AA7faVLeEEsh"
      },
      "source": [
        "## Removing links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm75bZvGEEsh"
      },
      "outputs": [],
      "source": [
        "pattern = r'https?://\\S+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMg71jAEEsn"
      },
      "source": [
        "## Remove foriegn words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1EjRxMEEsn"
      },
      "source": [
        "The text includes english, japanese and words for other languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HraPS5_SEEso"
      },
      "outputs": [],
      "source": [
        "pattern = r'[a-zA-Z]+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_r3f6p9DEEsp"
      },
      "source": [
        "## Remove punctuations & special chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2gtB69BEEsp"
      },
      "outputs": [],
      "source": [
        "pattern = r'[^\\w\\s\\u0600-\\u06FF]+|_|ï·º|Û©|â“µ|ØŸ|Ø›|Û|ï·»|ØŒ| Ù°'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-BEPhEhYEEsq"
      },
      "source": [
        "## Remove consecutive characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjEAl55ZEEsr"
      },
      "outputs": [],
      "source": [
        "pattern = r'(.)\\1+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, r'\\1', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove tatweel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_tatweel(document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MZctqr8qEEsr"
      },
      "source": [
        "## Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUfiKGlxEEst"
      },
      "outputs": [],
      "source": [
        "pattern = r'\\d+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove extra whitespaces\n",
        "In this step we get rid of extra whitespaces as well as new lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pattern = r'\\s+|\\n+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove harakat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_tashkeel(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove diactrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_diacritics(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize hamza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pattern = r\"Ø£|Ø¥|Ø¢\"\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, 'Ø§', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sy4GggUEEsw"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nf_D_MeEEsx"
      },
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.tokenize(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove long & short words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: [word for word in document if len(word) < 9 and len(word) > 1])\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pQOiy36-EEsy"
      },
      "source": [
        "## Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFgkKc6GEEsy"
      },
      "outputs": [],
      "source": [
        "ar_stemmer = stemmer(\"arabic\")\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda doc: [ar_stemmer.stemWord(token) for token in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arabic_stopwords = stopwords.words('arabic')\n",
        "arabic_stopwords.extend(stp.stopwords_list())\n",
        "stop_words = {ar_stemmer.stemWord(entry) for entry in arabic_stopwords}\n",
        "with open(\"arabic_stopwords.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    for word in file:\n",
        "        stop_words.add(ar_stemmer.stemWord(word.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "open(\"./models/stopwords.pkl\", \"wb\").write(pickle.dumps(stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_stopwords(document: str) -> str:\n",
        "    words = set(document.split(\" \"))\n",
        "    return \" \".join(list(words - stop_words))\n",
        "\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: \" \".join([token for token in document if token not in stop_words]))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw0Z0MsXEEsz"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"preprocessed_data.csv\"):\n",
        "    # remove empty entries\n",
        "    # data.replace('', pd.NA, inplace=True)  # Replace empty strings with NA\n",
        "    # data.dropna(inplace=True)  # Drop rows with NA values\n",
        "    data.to_csv(\"preprocessed_data.csv\") # inspect the resulting file to validate the preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l4AEbbxhEEsz"
      },
      "source": [
        "# Text representation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appraisal analysis\n",
        "Bow + G:AO\n",
        "\n",
        "G:AO Appraisal Group by Attitude & Orientation â€” Total\n",
        "frequency of appraisal groups with each possible combination of Attitude and Orientation, normalized by total number of appraisal groups in the text."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openpyxl\n",
        "\n",
        "def excel_to_dict(file_path):\n",
        "    workbook = openpyxl.load_workbook(file_path)\n",
        "    data_dict = {}\n",
        "    \n",
        "    for sheet_name in workbook.sheetnames:\n",
        "        sheet = workbook[sheet_name]\n",
        "        values = [cell.value for cell in sheet['A'] if cell.value is not None]\n",
        "        data_dict[sheet_name] = values\n",
        "    \n",
        "    return data_dict\n",
        "\n",
        "# Example usage:\n",
        "file_path = 'example.xlsx'  # Replace 'example.xlsx' with your file path\n",
        "lexicon = excel_to_dict(\"./Arabic seed terms.xlsx\")\n",
        "print(lexicon)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess lexicon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'affect_happ_neg': ['Ø­Ø²ÙŠÙ† ', 'Ù‚Ø§Ù†Ø·', 'Ø¬Ø²Ø¹', 'Ù…Ø­Ø·Ù… ', 'ÙŠÙŠØ¨ ', 'Ù…Ø­Ø¨Ø·', 'Ù…ÙˆÙ„Ù…', 'Ù…Ù†Ù‚Ø¨Ø¶', 'Ù…Ø«ÙŠØ± Ù„Ù„Ø´ÙÙ‚', 'Ù…Ù‡Ù…ÙˆÙ…', 'Ù…ÙƒØªÙŠØ¨', 'Ù…ÙƒØ¯Ø±', 'Ù…Ù…Ù„', 'Ø¨Ø§ÙŠØ³', 'ØªØ¹ÙŠØ³', 'Ù…Ø­Ø²', 'Ù‚Ø§ØªÙ…', 'Ù…ØªØ¬', 'Ù…Ù†ÙƒØ³Ø± Ø§Ù„Ø®Ø§Ø·Ø±', 'Ù…Ù†Ø¹Ø²Ù„', 'Ù…ØªØ¯', 'Ù…ØºØªÙ…', 'ÙƒØ§Ø¡', 'Ø¨Ø§Ùƒ', 'Ø¯Ø§Ù…Ø¹'], 'affect_happ_pos': ['Ù…Ø±Ø­', 'Ù†Ø´Ø·', 'Ù…Ø¨ØªÙ‡Ø¬'], 'affect_inc_neg': ['Ø­Ø°Ø±', 'Ø®Ø§ÙŠÙ', 'Ù…ÙØ²ÙˆØ¹'], 'affect_satis_neg': ['Ø³Ø·Ø­', 'Ù…Ø¨ØªØ°Ù„', 'Ù…Ù…Ù„', 'Ù…Ù†Ø²Ø¹Ø¬', 'ØºØ§Ø¶Ø¨', 'Ù…ØºÙŠØ¸', 'Ù…ØªØ¶Ø§ÙŠÙ‚', 'Ø³Ø§Ø®Ø·', 'Ø³ÙŠÙŠÙ…'], 'affect_satis_pos': ['Ù…Ø¹Ù†', 'Ù…Ù†Ø´ØºÙ„ ', 'Ù…Ù†Ù‡Ù…', 'Ø±Ø§Ø¶', 'Ù…Ø³Ø±ÙˆØ±', 'Ù…Ø¹Ø¬Ø¨ ', 'Ø³Ø¹ÙŠØ¯', 'Ù…Ù†Ø¨Ù‡Ø±', 'Ù…ÙØªÙˆÙ†', 'Ù…Ø«ÙŠØ±', 'Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ø¹Ø¬Ø§Ø¨'], 'affect_sec_neg': ['Ù…Ø±ØªØ¨', 'Ù‚Ù„Ù‚ ', 'Ø´Ø§Ø°', 'Ù…ÙØ§Ø¬ÙŠ ', 'Ù…Ù†Ø¯Ù‡Ø´', 'Ù…Ø´Ø¯'], 'affect_sec_pos': ['ÙˆØ§Ø«Ù‚', 'Ù…ÙˆÙƒØ¯', 'Ù…Ø±ÙŠØ­', 'ÙˆØ§Ø«Ù‚ Ù…Ù† Ù†ÙØ³', 'Ù…ÙˆØ¶Ø¹ Ø«Ù‚'], 'apprec_comp_balance_neg': ['Ù…Ø®ØªÙ„', 'Ù…ØªØ¹Ø§Ø±Ø¶', 'Ù…ØªÙ‚Ø·Ø¹', 'Ù…ØªÙØ§', 'Ù…Ø¹ÙŠØ¨', 'Ù…ØªÙ†Ø§Ù‚Ø¶', 'ÙˆØ¶Ùˆ', 'Ù…Ø´Ùˆ', 'Ø¨Ø´Ø¹', 'Ù…Ø­Ø±Ù'], 'apprec_comp_balance_pos': ['Ù…ØªÙˆØ§Ø²', 'Ù…ØªÙ†Ø§ØºÙ…', 'Ù…ÙˆØ­Ø¯', 'Ù…ØªÙ…Ø§Ø«Ù„', 'Ù…ØªÙ†Ø§Ø³Ø¨', 'Ù…Ù„Ø§ÙŠÙ…', 'Ù…Ø­ØªØ±Ù…', 'Ù…Ù†Ø·Ù‚ÙŠ ', 'Ù…ØªÙ†Ø§Ø³Ù‚', 'Ø±Ø´ÙŠÙ‚', 'Ù…Ø±ØªØ¨'], 'apprec_comp_complex_neg': ['Ù…Ø¹Ù‚Ø¯', 'Ù…ÙØ±Ø·', 'ÙŠØ²Ù†Ø·', 'ØºØ§Ù…Ø¶', 'Ù…Ø¨Ù‡Ù…', 'Ø¹ÙƒØ± ', 'Ø¹Ø§Ø¯', 'Ø§Ø¹ØªÙŠØ§Ø¯', 'Ù…Ø¨Ø³Ø·'], 'apprec_comp_complex_pos': ['Ø³ÙŠØ·', 'ØµØ§Ù', 'Ø§Ù†ÙŠÙ‚ ', 'Ø´ÙØ§Ù', 'ÙˆØ§Ø¶Ø­', 'Ø¯Ù‚ÙŠÙ‚ ', 'Ù…Ø±ÙƒØ¨', 'ØºÙ†', 'Ù…ÙØµÙ„'], 'apprec_reaction_impact_neg': ['Ø¨Ø§Ù‡', 'Ù…Ù…Ù„', 'Ù…Ø¶Ø¬Ø± ', 'Ø¬Ø§Ù', 'ØºØ§Ù…Ø¶', 'Ø³Ù‚ÙŠÙ…', 'Ø³Ø·Ø­', 'Ù…ØªÙˆÙ‚Ø¹', 'Ø±ØªÙŠØ¨', 'Ø®Ø§Ù', 'ØªØ§Ù'], 'apprec_reaction_impact_pos': ['Ø§ÙØª Ù„Ù„Ù†Ø¸Ø±', 'Ø®Ù„Ø§Ø¨', 'Ø¬Ø°Ø§Ø¨ ', 'ÙØ§ØªÙ†', 'Ù…Ø«ÙŠØ±', 'Ù…ÙˆØ«Ø±', 'Ù†Ø´ÙŠØ·', 'Ù…Ø°Ù‡Ù„', 'Ø­Ø§Ø¯', 'Ø±Ø§ÙŠØ¹', 'Ø¨Ø§Ø±Ø²', 'Ø­Ø³Ø§Ø³'], 'apprec_reaction_quality_neg': ['Ø³ÙŠØ¡', 'ÙƒØ±', 'Ù…Ù‚Ø±Ù ', 'Ø¹Ø§Ø¯', 'Ù‚Ø¨ÙŠØ­', 'Ø´Ø¹ ', 'ØºÙŠØ¶', 'ØºØ§Ø¶Ø¨', 'Ù…Ø´Ù…ÙŠØ²'], 'apprec_reaction_quality_pos': ['Ù…Ù‚Ø¨ÙˆÙ„', 'Ø±ÙÙŠØ¹', 'Ø¬ÙŠØ¯', 'Ù…Ø­Ø¨ÙˆØ¨', 'Ø¬Ù…ÙŠÙ„', 'Ø±Ø§ÙŠØ¹', 'Ø¬Ø°Ø§Ø¨', 'ÙØ§ØªÙ†', 'Ù…Ø­ØªÙ'], 'apprec_valuation_neg': ['Ø¶Ø­Ù„', 'Ù…Ø®ØªØ²Ù„', 'ØªØ§Ù', 'Ø«Ø§Ù†Ùˆ', 'ØªÙ‚Ù„ÙŠØ¯', 'Ø±Ùƒ', 'Ù…Ø­Ø§ÙØ¸', 'Ù…ØªØ§Ø®Ø±', 'Ù‚Ø¯ÙŠÙ…', 'Ø´Ø§ÙŠØ¹', 'Ù…Ø¨ØªØ°Ù„', 'Ø¹Ø§Ø¯', 'Ø²Ø§ÙŠÙ', 'ÙƒØ§Ø°Ø¨', 'Ù…ØºØ±', 'Ø±Ø®ÙŠØµ', 'Ù…Ø²ÙŠÙ', 'ØºØ§Ù„', 'Ø¹Ø§Ø¬Ø² ', 'Ø¹Ù‚ÙŠÙ…', 'Ø¨Ø§Ø·Ù„'], 'apprec_valuation_pos': ['Ù†Ø§ÙØ°', 'Ø¹Ù…ÙŠÙ‚', 'Ø±Ø§Ø³Ø®', 'Ù…Ø¨ØªÙƒØ±', 'Ø§ØµÙŠÙ„', 'Ø®Ù„Ø§Ù‚', 'Ù…Ù†Ø§Ø³Ø¨', 'Ù…Ù†ØªØ¸Ø±', 'Ù…ÙˆØ´Ø±', 'Ù…Ù…ÙŠØ²', 'Ø§Ø³ØªØ«Ù†Ø§Ø¡', 'Ø±ÙŠØ¯', 'Ø§ØµÙ„', 'Ø­Ù‚ÙŠÙ‚ÙŠ ', 'Ù†ÙÙŠØ³', 'Ø«Ù…ÙŠÙ†', 'Ù…Ù‡Ù…', 'Ù…Ù„Ø§ÙŠÙ…', 'Ù…ÙÙŠØ¯', 'Ø¹Ø§Ù„'], 'judg_esteem_cap_neg': ['Ù…ØªØ³Ø§Ù…Ø­', 'Ø¶Ø¹ÙŠÙ', 'Ø¬Ø¨Ø§Ù†', 'ÙØ§Ø³Ø¯', 'Ù…Ø±ÙŠØ¶', 'Ø§Ø¹Ø±Ø¬', 'ÙØ¬', 'ØµØ¨', 'Ø¹Ø§Ø¬Ø²', 'Ù…Ù…Ù„', 'ÙƒÙŠÙŠØ¨', 'Ø±Ù‡ÙŠØ¨', 'Ø·ÙŠØ¡', 'ØºØ¨', 'Ø«Ù‚ÙŠÙ„ ', 'Ø§Ø¨Ù„', 'Ø¹ØµØ§Ø¨', 'Ù…Ø®ØªÙ„', 'Ø³Ø§Ø°Ø¬', 'Ù‡Ø§Ùˆ', 'Ø§Ø­Ù…Ù‚', 'Ø§Ù…', 'Ø®Ø´Ù†', 'Ø¬Ø§Ù‡Ù„ ', 'Ø¹Ø§Ø¬Ø²', 'Ù†Ø§Ù‚Øµ', 'Ù…Ø®ÙÙ‚', 'Ø¹Ù‚ÙŠÙ…'], 'judg_esteem_cap_pos': ['Ù‚ÙˆÙŠ ', 'Ø­ÙŠÙˆ', 'Ù…ØªÙŠÙ† ', 'Ø³Ù„ÙŠÙ…', 'ØµØ­', 'ØµØ§Ù„Ø­ ', 'Ø¨Ø§Ù„Øº', 'Ù†Ø§Ø¶Ø¬', 'Ù…Ø¬Ø±Ø¨ ', 'Ø¨Ø§Ø±Ø¹', 'Ù‡Ø²Ù„', 'Ø·Ø±ÙŠÙ ', 'Ø«Ø§Ù‚Ø¨', 'Ø°Ùƒ', 'Ù…ÙˆÙ‡ÙˆØ¨', 'Ù…ØªÙˆØ§Ø²', 'ÙˆØ§Ø«Ù‚', 'Ø¹Ø§Ù‚Ù„', 'Ù…Ø¹Ù‚ÙˆÙ„', 'Ø®Ø¨ÙŠØ±', 'Ù…Ø§Ù‡Ø±', 'Ø¹Ø§Ø±Ù', 'Ù…Ù‡Ø°Ø¨', 'Ù…ØªØ¹Ù„Ù…', 'ÙƒÙØ¡', 'Ù…ØªÙ…ÙƒÙ† ', 'Ù†Ø§Ø¬Ø­', 'Ù…Ù†ØªØ¬'], 'judg_esteem_norm_neg': ['Ù…Ø´ÙˆÙˆÙ…', 'Ø¨Ø§ÙŠØ³', 'Ù…Ù†Ø­ÙˆØ³  ', 'ØºØ±ÙŠØ¨', 'Ø¹Ø¬ÙŠØ¨', 'Ù…ØªÙ‚Ù„Ø¨', 'ØªØ§Ø¡', 'ØºØ§Ù…Ø¶ ', 'Ù…Ø­Ø§ÙØ¸', 'Ù‚Ø¯ÙŠÙ…', 'Ø±Ø¬Ø¹', 'Ù…Ø¨Ù‡Ù…', 'ÙØ§Ø´Ù„'], 'judg_esteem_norm_pos': ['Ù…Ø­Ø¸ÙˆØ¸', 'Ù…ÙˆÙÙ‚', 'Ø±Ø§ÙŠØ¹', 'Ø¹Ø§Ø¯', 'Ø·Ø¨ÙŠØ¹', 'Ù…Ø§Ù„ÙˆÙ ', 'Ù‡Ø§Ø¯Ø¡', 'Ù…Ø³ØªÙ‚Ø±', 'Ù…ØªÙˆÙ‚Ø¹ ', 'Ø±Ø§Ù‡', 'Ø¹ØµØ±', 'Ø·Ù„ÙŠØ¹', 'Ù…Ø´Ù‡ÙˆØ±', 'Ù…ØºÙ…ÙˆØ±'], 'judg_esteem_ten_neg': ['Ø®Ø¬ÙˆÙ„', 'Ø¬Ø¨Ø§Ù†', 'Ø®ÙˆØ§Ù ', 'Ù…Ù†Ø¯ÙØ¹', 'Ø¨Ø±Ù…', 'Ù…ØªÙ‡ÙˆØ±', 'Ù…ØªØ³Ø±Ø¹', 'Ù…ØªÙ‚Ù„Ø¨', 'Ø·Ø§ÙŠØ´ ', 'Ø¶Ø¹ÙŠÙ', 'ØºØ§ÙÙ„', 'Ø¬Ø²Ø¹  ', 'Ø®Ø§ÙŠÙ† ', 'Ù…Ø®Ø§Ø¯Ø¹ ', 'ØºØ§Ø¯Ø±', 'Ù…ØªØºÙŠØ±', 'Ø¹Ù†ÙŠØ¯', 'Ù…ØªØ¹', 'Ù…ØªØ¹Ù…Ø¯'], 'judg_esteem_ten_pos': ['Ù…Ù‚Ø¯Ø§Ù…', 'Ø´Ø¬Ø§Ø¹', 'Ø·ÙˆÙ„ÙŠ  ', 'Ø­Ø°Ø±', 'Ù…Ø­ØªØ±Ø³', 'ØµØ§Ø¨Ø±', 'ÙŠÙ‚Ø¸', 'ÙƒØ§Ù…Ù„', 'Ø¯Ù‚ÙŠÙ‚', 'Ù†Ø´ÙŠØ·', 'Ù…Ø«Ø§Ø¨Ø±', 'Ø­Ø§Ø²Ù…', 'Ù…ÙˆØ«ÙˆÙ‚', 'Ù…Ø¹ØªÙ…Ø¯ ', 'Ù…Ø®Ù„Øµ', 'ÙˆÙ', 'Ø«Ø§Ø¨', 'Ù…Ø±Ù†', 'Ù…ØªÙƒÙŠÙ ', 'Ù…ØªØ¹'], 'judg_sanction_prop_neg': ['Ø³ÙŠØ¡', 'Ø³Ø§ÙÙ„', 'Ø´Ø±ÙŠØ± ', 'Ù…Ø±ØªØ´', 'Ø¸Ø§Ù„Ù…', 'Ø¬Ø§ÙŠØ± ', 'Ù‚Ø§Ø³', 'Ø®Ø³ÙŠØ³', 'Ø´Ø±Ø³', 'ØªØ§Ù', 'Ù…ØªÙƒØ¨Ø±', 'Ù…ØªØºØ·Ø±Ø³', 'ÙˆÙ‚Ø­', 'ÙØ¸', 'Ù…Ø­ØªÙ‚Ø±', 'Ø§Ù†Ø§', 'Ø¬Ø´Ø¹', 'Ø®ÙŠÙ„'], 'judg_sanction_prop_pos': ['ØµØ§Ù„Ø­', 'Ù…Ø¹Ù†Ùˆ', 'Ø§Ø®Ù„Ø§Ù‚ÙŠ ', 'Ù…Ù„ØªØ²Ù… Ø¨Ø§Ù„Ù‚', 'Ø¹Ø§Ø¯Ù„', 'Ù…Ù†ØµÙ', 'Ù…Ø±Ù‡Ù', 'Ù„Ø·ÙŠÙ', 'Ø­Ø±ÙŠØµ  ', 'Ø³ÙŠØ·', 'Ù…ØªÙˆØ§Ø¶Ø¹', 'Ø¯ÙŠØ¹  ', 'Ù…Ù‡Ø°Ø¨', 'Ù…Ø­ØªØ±Ù…', 'Ù…ÙˆÙ‚Ø±', 'Ø§ÙŠØ«Ø§Ø±', 'Ø³Ø®', 'Ù…Ø­Ø³'], 'judg_sanction_ver_neg': ['ØºØ´Ø§Ø´', 'Ù…Ø®Ø§Ø¯Ø¹', 'Ø°Ø§Ø¨ ', 'Ù…Ø¶Ù„Ù„', 'Ù…ØªÙ„Ø§Ø¹Ø¨', 'Ù…Ø±Ø§ÙˆØº', 'ÙƒÙ„ÙŠÙ„', 'Ø«Ø±Ø«Ø§Ø±'], 'judg_sanction_ver_pos': ['ØµØ§Ø¯Ù‚', 'Ù†Ø²', 'Ù…ÙˆØ«ÙˆÙ‚ ', 'ØµØ±ÙŠØ­', 'ÙˆØ§Ø¶Ø­', 'Ù…Ø¨Ø§Ø´Ø±  ', 'Ù…Ø­ØªØ±Ø³', 'Ù„Ø¨Ù‚'], 'verb_affect_happ_neg': ['ÙƒØ±Ù‡', 'Ø§Ø¨ØºØ¶', 'Ù…Ù‚Øª'], 'verb_affect_happ_pos': ['Ù…Ø§Ù„', 'Ø­Ø¨', 'Ø¹Ø´Ù‚', 'ÙˆÙ„Ø¹'], 'verb_affect_inc_pos': ['Ø§ÙØªÙ‚Ø¯', 'Ø§Ø´ØªØ§Ù‚', 'ØªØ§Ù‚']}\n"
          ]
        }
      ],
      "source": [
        "ar_stemmer = stemmer(\"arabic\")\n",
        "for key, val in lexicon.items():\n",
        "    for idx, val in enumerate(val):\n",
        "        preprocessed_val = araby.strip_tashkeel(val)\n",
        "        preprocessed_val = araby.strip_diacritics(preprocessed_val)\n",
        "        preprocessed_val = re.sub(r\"Ø£|Ø¥|Ø¢\", 'Ø§', preprocessed_val)\n",
        "        preprocessed_val = ar_stemmer.stemWord(preprocessed_val)\n",
        "        lexicon[key][idx] = preprocessed_val\n",
        "print(lexicon)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appraisal features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construct a word to appraisal group mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_to_appraisal_grp = {}\n",
        "for key, val in lexicon.items():\n",
        "    for values in val:\n",
        "        word_to_appraisal_grp[values] = key\n",
        "word_to_appraisal_grp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "appraisal_grp_to_idx = {}\n",
        "idx = 0\n",
        "for appraisal_grp in list(lexicon.keys()):\n",
        "    appraisal_grp_to_idx[appraisal_grp] = idx\n",
        "    idx += 1\n",
        "appraisal_grp_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def appraisal_features(document: str) -> List[float]:\n",
        "    res = np.zeros(len(lexicon))\n",
        "    tokens = [word for word in document.split(\" \")]\n",
        "    count_appraisal_grps = 0\n",
        "    for token in tokens:\n",
        "        if token in word_to_appraisal_grp:\n",
        "            res[ appraisal_grp_to_idx[word_to_appraisal_grp[token]] ] += 1\n",
        "            count_appraisal_grps += 1\n",
        "    # normalize features by the count of appraisal groups if the count != 0\n",
        "    res = res / count_appraisal_grps if count_appraisal_grps != 0 else res\n",
        "    return res\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(24742,)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify = data[\"class\"])\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train_BOW = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_BOW = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "X_train_BOW[0].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "construct training and testing appraisal feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(23642, 30)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_appraisal = np.array(\n",
        "    [appraisal_features(document) for document in X_train]\n",
        ")\n",
        "\n",
        "X_train_appraisal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5911, 30)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test_appraisal = np.array(\n",
        "    [appraisal_features(document) for document in X_test]\n",
        ")\n",
        "\n",
        "X_test_appraisal.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Features' union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(23642, 24772)\n",
            "(5911, 24772)\n"
          ]
        }
      ],
      "source": [
        "X_train = np.hstack((X_train_BOW, X_train_appraisal))\n",
        "X_test  = np.hstack((X_test_BOW, X_test_appraisal))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance evaluation\n",
        "## Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 58.53%\n",
            "Precesion : 55.61%\n",
            "Recall : 75.55%\n",
            "F1 score : 64.07%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/APPRAISAL/NB.pkl\"):\n",
        "    # read model from disk\n",
        "    model = open(\"./models/ASTC/APPRAISAL/NB.pkl\", 'rb').read()\n",
        "    model: GaussianNB = pickle.loads(model)\n",
        "else:\n",
        "    model = GaussianNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    # write model to disk\n",
        "    mdl_bytes = pickle.dumps(model)\n",
        "    open(\"./models/ASTC/APPRAISAL/NB.pkl\", 'wb').write(mdl_bytes)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_N8hCmIEEtK",
        "outputId": "7fd72e2e-c809-445a-851d-035c5e8381eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 87.50%\n",
            "Precesion : 85.21%\n",
            "Recall : 90.08%\n",
            "F1 score : 87.58%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/APPRAISAL/LR.pkl\"):\n",
        "    # read model from disk\n",
        "    model = open(\"./models/ASTC/APPRAISAL/LR.pkl\", 'rb').read()\n",
        "    model: LogisticRegression = pickle.loads(model)\n",
        "else:\n",
        "    model = LogisticRegression(random_state = SEED, max_iter = 1500)\n",
        "    model.fit(X_train, y_train)\n",
        "    # write model to disk\n",
        "    mdl_bytes = pickle.dumps(model)\n",
        "    open(\"./models/ASTC/APPRAISAL/LR.pkl\", 'wb').write(mdl_bytes)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM\n",
        "\n",
        "We were unable to train this model on our machines using the initial dataset, due to the **curse of dimensionality**, so we added dimensioanlity reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 86.84%\n",
            "Precesion : 82.48%\n",
            "Recall : 92.81%\n",
            "F1 score : 87.34%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/APPRAISAL/SVM_Pipeline.pkl\"):\n",
        "    # read pipeline from disk\n",
        "    pipeline = open(\"./models/ASTC/APPRAISAL/SVM_Pipeline.pkl\", 'rb').read()\n",
        "    pipeline = pickle.loads(pipeline)\n",
        "else:\n",
        "    steps = [\n",
        "        ('pca', PCA(n_components = 150, random_state = SEED)),\n",
        "        ('svm', SVC(random_state = SEED))\n",
        "    ]\n",
        "    pipeline = Pipeline(steps = steps)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    # write pipeline to disk\n",
        "    pipeline_bytes = pickle.dumps(pipeline)\n",
        "    open(\"./models/ASTC/APPRAISAL/SVM_Pipeline.pkl\", 'wb').write(pipeline_bytes)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label = \"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 86.84%\n",
            "Precesion : 82.48%\n",
            "Recall : 92.81%\n",
            "F1 score : 87.34%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/APPRAISAL/RF.pkl\"):\n",
        "    # read model from disk\n",
        "    model = open(\"./models/ASTC/APPRAISAL/RF.pkl\", 'rb').read()\n",
        "    model = pickle.loads(model)\n",
        "else:\n",
        "    model = RandomForestClassifier(random_state = SEED)\n",
        "    model.fit(X_train, y_train)\n",
        "    # write model to disk\n",
        "    mdl_bytes = pickle.dumps(model)\n",
        "    open(\"./models/ASTC/APPRAISAL/RF.pkl\", 'wb').write(mdl_bytes)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
