{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OtmaneDaoudi/Arabic-sentiment-analysis/blob/main/arabic_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YCMH2OjBGA8u"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BqPG4jsGF8m",
        "outputId": "71519abe-6ccb-4059-c163-edb45164bcbe"
      },
      "outputs": [],
      "source": [
        "# !pip install emoji\n",
        "# !pip install Arabic-Stopwords\n",
        "# !pip install seaborn\n",
        "# !pip install matplotlib\n",
        "# !pip install soyclustering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KUh0GecqEErv"
      },
      "source": [
        "# Libs imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sQdYY3KPEEsC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "import emoji\n",
        "import pickle\n",
        "\n",
        "import arabicstopwords.arabicstopwords as stp\n",
        "import pandas as pd\n",
        "import pyarabic.araby as araby\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "from snowballstemmer import stemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "SEED = 21"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "MvI6l3cnEEsH",
        "outputId": "840f53a3-1120-4709-a395-10f90977c0a9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>⛔ طريق #الدمام × #الخبر ابعدواا عنه 📣</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح 😂</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء 💕 شاطئ فلامنغو ، أروبا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44109</th>\n",
              "      <td>pos</td>\n",
              "      <td>ايوه صح بس ماتاخذني لمكان بعيد زيه 😂</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24380</th>\n",
              "      <td>pos</td>\n",
              "      <td>صباحك خير وبركة نجاة 🌺 💗 🌹</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23661</th>\n",
              "      <td>pos</td>\n",
              "      <td>تحية إلي أهل #ليبيا الكل وياعن دين زك أم اللي ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39242</th>\n",
              "      <td>pos</td>\n",
              "      <td>ماعرفتك وانت لبناني 😅</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42630</th>\n",
              "      <td>pos</td>\n",
              "      <td>سبحان الله الحمد لله لاإله الاالله الله أكبر ل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22367</th>\n",
              "      <td>neg</td>\n",
              "      <td>راح بعثره وجاء فهد وطر جيبي 😯</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34645</th>\n",
              "      <td>pos</td>\n",
              "      <td>بمناسبة فوز الهلال .. 💙 سحب على آيفون XR📱 رتوي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29356</th>\n",
              "      <td>pos</td>\n",
              "      <td>وضع الدوري هالسنه 😁</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>neg</td>\n",
              "      <td>ع اي اش اله علاقة بالطول 🌚</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44732</th>\n",
              "      <td>pos</td>\n",
              "      <td>اغبى موقف حلمانه بشخص وتقعدين تدورين عليه 😀</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3552</th>\n",
              "      <td>neg</td>\n",
              "      <td>عبدالله بالله ريتويت صدقة عن امي في هذا الليلة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4152</th>\n",
              "      <td>neg</td>\n",
              "      <td>واحد تبع النظام السوري يقول أن المخابرات السور...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26120</th>\n",
              "      <td>pos</td>\n",
              "      <td>كلمه وحده تعبر عن شعوري حاليا: وييعع 🙂</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24570</th>\n",
              "      <td>pos</td>\n",
              "      <td>حسب توقيتكم انا مواصل ما نمت لما انام واصحى يص...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19035</th>\n",
              "      <td>neg</td>\n",
              "      <td>تبا ل خفاق مايرضا ب أحد غيرك كنه عشانك من ديار...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                              tweet\n",
              "33372   pos         بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos        شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg              ⛔ طريق #الدمام × #الخبر ابعدواا عنه 📣\n",
              "37264   pos                                        احلى صباح 😂\n",
              "35690   pos             مع بعض الأصدقاء 💕 شاطئ فلامنغو ، أروبا\n",
              "44109   pos               ايوه صح بس ماتاخذني لمكان بعيد زيه 😂\n",
              "24380   pos                         صباحك خير وبركة نجاة 🌺 💗 🌹\n",
              "23661   pos  تحية إلي أهل #ليبيا الكل وياعن دين زك أم اللي ...\n",
              "39242   pos                              ماعرفتك وانت لبناني 😅\n",
              "42630   pos  سبحان الله الحمد لله لاإله الاالله الله أكبر ل...\n",
              "22367   neg                      راح بعثره وجاء فهد وطر جيبي 😯\n",
              "34645   pos  بمناسبة فوز الهلال .. 💙 سحب على آيفون XR📱 رتوي...\n",
              "29356   pos                                وضع الدوري هالسنه 😁\n",
              "7236    neg                         ع اي اش اله علاقة بالطول 🌚\n",
              "44732   pos        اغبى موقف حلمانه بشخص وتقعدين تدورين عليه 😀\n",
              "3552    neg  عبدالله بالله ريتويت صدقة عن امي في هذا الليلة...\n",
              "4152    neg  واحد تبع النظام السوري يقول أن المخابرات السور...\n",
              "26120   pos             كلمه وحده تعبر عن شعوري حاليا: وييعع 🙂\n",
              "24570   pos  حسب توقيتكم انا مواصل ما نمت لما انام واصحى يص...\n",
              "19035   neg  تبا ل خفاق مايرضا ب أحد غيرك كنه عشانك من ديار..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"./datasets/ASTC/data.tsv\", header = 0, sep='\\t', names = [\"class\", \"tweet\"]).sample(frac = 1, random_state = SEED)\n",
        "data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fqggWownEEsK",
        "outputId": "e8a398e4-bfd2-44cf-b85f-2ea0ab850aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 45274 entries, 33372 to 15305\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   class   45274 non-null  object\n",
            " 1   tweet   45274 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GIECLLDlEEsM"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NDtOpDzdEEsP"
      },
      "source": [
        "Our preprocessing pipeline contains the following steps:\n",
        "\n",
        "1.  Remove duplicat entries\n",
        "2.  Replacing emojies & emoticons\n",
        "3.  Remove mentions\n",
        "4.  Remove Links\n",
        "5.  Remove whitespaces\n",
        "6.  Remove punctuations & Special chars\n",
        "7.  Remove Consecutive characters\n",
        "8.  Tokenization\n",
        "9.  Remove foreign words\n",
        "10. Remove stop words\n",
        "11. Remove numbers\n",
        "12. Stemming\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zolKP03vEEsT"
      },
      "source": [
        "## Removing duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeMI9vGpEEsW",
        "outputId": "e2fb0124-2a17-4db2-c06f-52d48a0b7000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.7% of the data are duplicats\n"
          ]
        }
      ],
      "source": [
        "count = data.duplicated().sum()\n",
        "print(f\"{(count / data.shape[0]) * 100:.1f}% of the data are duplicats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ahlc7NfGEEsY"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(inplace = True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv54Q2aKEEsZ"
      },
      "source": [
        "## Replacing emojies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "75Dy9RLbEEsa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر  طريق #الدمام × #الخبر ابعدواا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب  شاطئ فلامنغو ، أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg      حذر  طريق #الدمام × #الخبر ابعدواا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos     مع بعض الأصدقاء حب  شاطئ فلامنغو ، أروبا"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emojis = {\n",
        "    \"🙂\":\"يبتسم\",\n",
        "    \"😂\":\"يضحك\",\n",
        "    \"🤣\" : \"يضحك\",\n",
        "    \"💔\":\"قلب حزين\",\n",
        "    \"🙂\":\"يبتسم\",\n",
        "    \"❤️\":\"حب\",\n",
        "    \"🥰\":\"حب\",\n",
        "    \"🤐\":\"سكوت\",\n",
        "    \"🧡\":\"حب\",\n",
        "    \"❤\":\"حب\",\n",
        "    \"😍\":\"حب\",\n",
        "    \"😭\":\"يبكي\",\n",
        "    \"🤭\":\"يبتسم\",\n",
        "    \"😢\":\"حزن\",\n",
        "    \"😔\":\"حزن\",\n",
        "    \"♥\":\"حب\",\n",
        "    \"💜\":\"حب\",\n",
        "    \"😅\":\"يضحك\",\n",
        "    \"🙁\":\"حزين\",\n",
        "    \"💕\":\"حب\",\n",
        "    \"💙\":\"حب\",\n",
        "    \"😞\":\"حزين\",\n",
        "    \"😊\":\"سعادة\",\n",
        "    \"👏\":\"يصفق\",\n",
        "    \"👌\":\"احسنت\",\n",
        "    \"😴\":\"ينام\",\n",
        "    \"😀\":\"يضحك\",\n",
        "    \"✅\":\"صحيح\",\n",
        "    \"🤪\":\"يضحك\",\n",
        "    \"🏡\" : \"بيت\",\n",
        "    \"🤲\" : \"دعاء\",\n",
        "    \"💰\" : \"مال\",\n",
        "    \"😌\":\"حزين\",\n",
        "    \"🎁\":\"هدية\",\n",
        "    \"🌹\":\"وردة\",\n",
        "    \"🥀\":\"وردة\",\n",
        "    \"📿\":\"وردة\",\n",
        "    \"✍\":\"كتابة\",\n",
        "    \"🙈\":\"حب\",\n",
        "    \"😄\":\"يضحك\",\n",
        "    \"😐\":\"محايد\",\n",
        "    \"✌\":\"منتصر\",\n",
        "    \"✨\":\"نجمه\",\n",
        "    \"🤔\":\"تفكير\",\n",
        "    \"😏\":\"يستهزء\",\n",
        "    \"😒\":\"يستهزء\",\n",
        "    \"🙄\":\"ملل\",\n",
        "    \"😕\":\"عصبية\",\n",
        "    \"😃\":\"يضحك\",\n",
        "    \"🌸\":\"وردة\",\n",
        "    \"😓\":\"حزن\",\n",
        "    \"💞\":\"حب\",\n",
        "    \"💗\":\"حب\",\n",
        "    \"😑\":\"منزعج\",\n",
        "    \"💭\":\"تفكير\",\n",
        "    \"😎\":\"ثقة\",\n",
        "    \"💛\":\"حب\",\n",
        "    \"😩\":\"حزين\",\n",
        "    \"🥺\":\"حزين\",\n",
        "    \"💪\":\"عضلات\",\n",
        "    \"👍\":\"موافق\",\n",
        "    \"🙏🏻\":\"رجاء طلب\",\n",
        "    \"😳\":\"مصدوم\",\n",
        "    \"👏🏼\":\"تصفيق\",\n",
        "    \"🎶\":\"موسيقي\",\n",
        "    \"🌚\":\"صمت\",\n",
        "    \"💚\":\"حب\",\n",
        "    \"🙏\":\"رجاء طلب\",\n",
        "    \"💘\":\"حب\",\n",
        "    \"🍃\":\"سلام\",\n",
        "    \"☺\":\"يضحك\",\n",
        "    \"🎊\":\"يهنئ\",\n",
        "    \"💥\":\"إنفجار\",\n",
        "    \"😝\":\"يسخر\",\n",
        "    \"💯\":\"تمام\",\n",
        "    \"🐸\":\"ضفدع\",\n",
        "    \"🤦‍♂️\":\"غبي\",\n",
        "    \"🤩\":\"معجب\",\n",
        "    \"🤤\":\"جائع\",\n",
        "    \"😶\":\"مصدوم\",\n",
        "    \"✌️\":\"مرح\",\n",
        "    \"✋🏻\":\"توقف\",\n",
        "    \"😉\":\"غمزة\",\n",
        "    \"🌷\":\"حب\",\n",
        "    \"🙃\":\"مبتسم\",\n",
        "    \"😫\":\"حزين\",\n",
        "    \"😨\":\"مصدوم\",\n",
        "    \"🎼 \":\"موسيقي\",\n",
        "    \"🍁\":\"مرح\",\n",
        "    \"🍂\":\"مرح\",\n",
        "    \"💟\":\"حب\",\n",
        "    \"😪\":\"حزن\",\n",
        "    \"😆\":\"يضحك\",\n",
        "    \"😣\":\"استياء\",\n",
        "    \"☺️\":\"حب\",\n",
        "    \"😱\":\"كارثة\",\n",
        "    \"😁\":\"يضحك\",\n",
        "    \"😖\":\"استياء\",\n",
        "    \"🏃🏼\":\"يجري\",\n",
        "    \"😡\":\"غضب\",\n",
        "    \"🚶\":\"يسير\",\n",
        "    \"🤕\":\"مرض\",\n",
        "    \"🤮\" : \"يتقيئ\",\n",
        "    \"⛔\": \"حذر\",\n",
        "    \"‼️\":\"تعجب\",\n",
        "    \"🕊\":\"طائر\",\n",
        "    \"👌🏻\":\"احسنت\",\n",
        "    \"❣\":\"حب\",\n",
        "    \"🙊\":\"مصدوم\",\n",
        "    \"💃\":\"سعادة مرح\",\n",
        "    \"💃🏼\":\"سعادة مرح\",\n",
        "    \"😜\":\"مرح\",\n",
        "    \"👊\":\"ضربة\",\n",
        "    \"😟\":\"استياء\",\n",
        "    \"💖\":\"حب\",\n",
        "    \"😥\":\"حزن\",\n",
        "    \"🎻\":\"موسيقي\",\n",
        "    \"✒\":\"يكتب\",\n",
        "    \"🚶🏻\":\"يسير\",\n",
        "    \"💎\":\"الماظ\",\n",
        "    \"😷\":\"وباء مرض\",\n",
        "    \"☝\":\"واحد\",\n",
        "    \"🚬\":\"تدخين\",\n",
        "    \"💐\" : \"ورد\",\n",
        "    \"🌻\" : \"ورد\",\n",
        "    \"🌞\" : \"شمس\",\n",
        "    \"👆\" : \"الاول\",\n",
        "    \"⚠️\" :\"تحذير\",\n",
        "    \"🤗\" : \"احتواء\",\n",
        "    \"✖️\": \"غلط\",\n",
        "    \"📍\"  : \"مكان\",\n",
        "    \"👸\" : \"ملكه\",\n",
        "    \"👑\" : \"تاج\",\n",
        "    \"✔️\" : \"صح\",\n",
        "    \"💌\": \"قلب\",\n",
        "    \"😲\" : \"مندهش\",\n",
        "    \"💦\": \"ماء\",\n",
        "    \"🚫\" : \"خطا\",\n",
        "    \"👏🏻\" : \"برافو\",\n",
        "    \"🏊\" :\"يسبح\",\n",
        "    \"👍🏻\": \"تمام\",\n",
        "    \"⭕️\" :\"دائره كبيره\",\n",
        "    \"🎷\" : \"ساكسفون\",\n",
        "    \"👋\": \"تلويح باليد\",\n",
        "    \"✌🏼\": \"علامه النصر\",\n",
        "    \"🌝\":\"مبتسم\",\n",
        "    \"➿\"  : \"عقده مزدوجه\",\n",
        "    \"💪🏼\" : \"قوي\",\n",
        "    \"📩\":  \"تواصل معي\",\n",
        "    \"☕️\": \"قهوه\",\n",
        "    \"😧\" : \"قلق و صدمة\",\n",
        "    \"🗨\": \"رسالة\",\n",
        "    \"❗️\" :\"تعجب\",\n",
        "    \"🙆🏻\": \"اشاره موافقه\",\n",
        "    \"👯\" :\"اخوات\",\n",
        "    \"©\" :  \"رمز\",\n",
        "    \"👵🏽\" :\"سيده عجوزه\",\n",
        "    \"🐣\": \"كتكوت\",\n",
        "    \"🙌\": \"تشجيع\",\n",
        "    \"🙇\": \"شخص ينحني\",\n",
        "    \"👐🏽\":\"ايدي مفتوحه\",\n",
        "    \"👌🏽\": \"بالظبط\",\n",
        "    \"⁉️\" : \"استنكار\",\n",
        "    \"⚽️\": \"كوره\",\n",
        "    \"🕶\" :\"حب\",\n",
        "    \"🎈\" :\"بالون\",\n",
        "    \"🎀\":    \"ورده\",\n",
        "    \"💵\":  \"فلوس\",\n",
        "    \"😋\":  \"جائع\",\n",
        "    \"😛\":  \"يغيظ\",\n",
        "    \"😠\":  \"غاضب\",\n",
        "    \"✍🏻\":  \"يكتب\",\n",
        "    \"🌾\":  \"ارز\",\n",
        "    \"👣\":  \"اثر قدمين\",\n",
        "    \"❌\":\"رفض\",\n",
        "    \"🍟\":\"طعام\",\n",
        "    \"👬\":\"صداقة\",\n",
        "    \"🐰\":\"ارنب\",\n",
        "    \"🦋\" : \"فراشة\",\n",
        "    \"☂\":\"مطر\",\n",
        "    \"⚜\":\"مملكة فرنسا\",\n",
        "    \"🐑\":\"خروف\",\n",
        "    \"🗣\":\"صوت مرتفع\",\n",
        "    \"👌🏼\":\"احسنت\",\n",
        "    \"☘\":\"مرح\",\n",
        "    \"😮\":\"صدمة\",\n",
        "    \"😦\":\"قلق\",\n",
        "    \"⭕\":\"الحق\",\n",
        "    \"✏️\":\"قلم\",\n",
        "    \"ℹ\":\"معلومات\",\n",
        "    \"🙍🏻\":\"رفض\",\n",
        "    \"⚪️\":\"نضارة نقاء\",\n",
        "    \"🐤\":\"حزن\",\n",
        "    \"💫\":\"مرح\",\n",
        "    \"💝\":\"حب\",\n",
        "    \"🍔\":\"طعام\",\n",
        "    \"❤︎\":\"حب\",\n",
        "    \"✈️\":\"سفر\",\n",
        "    \"🏃🏻‍♀️\":\"يسير\",\n",
        "    \"🍳\":\"ذكر\",\n",
        "    \"🎤\":\"مايك غناء\",\n",
        "    \"🎾\":\"كره\",\n",
        "    \"🐔\":\"دجاجة\",\n",
        "    \"🙋\":\"سؤال\",\n",
        "    \"📮\":\"بحر\",\n",
        "    \"💉\":\"دواء\",\n",
        "    \"🙏🏼\":\"رجاء طلب\",\n",
        "    \"💂🏿 \":\"حارس\",\n",
        "    \"🎬\":\"سينما\",\n",
        "    \"♦️\":\"مرح\",\n",
        "    \"💡\":\"قكرة\",\n",
        "    \"‼\":\"تعجب\",\n",
        "    \"👼\":\"طفل\",\n",
        "    \"🔑\":\"مفتاح\",\n",
        "    \"♥️\":\"حب\",\n",
        "    \"🌲\" : \"شجرة\",\n",
        "    \"🌳\" : \"شجرة\",\n",
        "    \"🚩\" : \"حذر\",\n",
        "    \"🚨\" : \"حذر\",\n",
        "    \"🛑\" : \"حذر\",\n",
        "    \"🕋\":\"كعبة\",\n",
        "    \"🐓\":\"دجاجة\",\n",
        "    \"💩\":\"معترض\",\n",
        "    \"👽\":\"فضائي\",\n",
        "    \"☔️\":\"مطر\",\n",
        "    \"🍷\":\"عصير\",\n",
        "    \"🌟\":\"نجمة\",\n",
        "    \"☁️\":\"سحب\",\n",
        "    \"👃\":\"معترض\",\n",
        "    \"🌺\":\"مرح\",\n",
        "    \"🔪\":\"سكينة\",\n",
        "    \"♨\":\"سخونية\",\n",
        "    \"👊🏼\":\"ضرب\",\n",
        "    \"✏\":\"قلم\",\n",
        "    \"🚶🏾‍♀️\":\"يسير\",\n",
        "    \"👊\":\"ضربة\",\n",
        "    \"◾️\":\"وقف\",\n",
        "    \"😚\":\"حب\",\n",
        "    \"🔸\":\"مرح\",\n",
        "    \"👎🏻\":\"لا يعجبني\",\n",
        "    \"👊🏽\":\"ضربة\",\n",
        "    \"😙\":\"حب\",\n",
        "    \"🎥\":\"تصوير\",\n",
        "    \"👉\":\"جذب انتباه\",\n",
        "    \"👏🏽\":\"يصفق\",\n",
        "    \"💪🏻\":\"عضلات\",\n",
        "    \"🏴\":\"اسود\",\n",
        "    \"🔥\":\"حريق\",\n",
        "    \"😬\":\"عدم الراحة\",\n",
        "    \"👊🏿\":\"يضرب\",\n",
        "    \"📚\" : \"كتب\",\n",
        "    \"📌\" : \"علق\",\n",
        "    \"🌿\":\"ورقه شجره\",\n",
        "    \"✋🏼\":\"كف ايد\",\n",
        "    \"👐\":\"ايدي مفتوحه\",\n",
        "    \"☠️\":\"وجه مرعب\",\n",
        "    \"🎉\":\"يهنئ\",\n",
        "    \"🔕\" :\"صامت\",\n",
        "    \"😿\":\"وجه حزين\",\n",
        "    \"☹️\":\"وجه يائس\",\n",
        "    \"😘\" :\"حب\",\n",
        "    \"😰\" :\"خوف و حزن\",\n",
        "    \"🌼\":\"ورده\",\n",
        "    \"💋\": \"بوسه\",\n",
        "    \"👇\":\"لاسفل\",\n",
        "    \"❣️\":\"حب\",\n",
        "    \"🎧\":\"سماعات\",\n",
        "    \"📝\":\"يكتب\",\n",
        "    \"😇\":\"دايخ\",\n",
        "    \"😈\":\"رعب\",\n",
        "    \"🏃\":\"يجري\",\n",
        "    \"✌🏻\":\"علامه النصر\",\n",
        "    \"🔫\":\"يضرب\",\n",
        "    \"❗️\":\"تعجب\",\n",
        "    \"👎\":\"غير موافق\",\n",
        "    \"🔐\":\"قفل\",\n",
        "    \"👈\":\"لليمين\",\n",
        "    \"™\":\"رمز\",\n",
        "    \"🚶🏽\":\"يتمشي\",\n",
        "    \"😯\":\"متفاجأ\",\n",
        "    \"✊\":\"يد مغلقه\",\n",
        "    \"😻\":\"اعجاب\",\n",
        "    \"🙉\" :\"قرد\",\n",
        "    \"👧\":\"طفله صغيره\",\n",
        "    \"🔴\":\"دائره حمراء\",\n",
        "    \"💪🏽\":\"قوه\",\n",
        "    \"💤\":\"ينام\",\n",
        "    \"👀\":\"ينظر\",\n",
        "    \"✍🏻\":\"يكتب\",\n",
        "    \"❄️\":\"تلج\",\n",
        "    \"💀\":\"رعب\",\n",
        "    \"😤\":\"وجه عابس\",\n",
        "    \"🖋\":\"قلم\",\n",
        "    \"🎩\":\"كاب\",\n",
        "    \"☕️\":\"قهوه\",\n",
        "    \"😹\":\"ضحك\",\n",
        "    \"💓\":\"حب\",\n",
        "    \"☄️\":\"نار\",\n",
        "    \"👻\":\"رعب\",\n",
        "    \"✋\": \"يد\",\n",
        "    \"🌱\": \"نبتة\",\n",
        "\n",
        "    # Emoticons\n",
        "    \":)\" : \"يبتسم\",\n",
        "    \"(:\" : \"يبتسم\",\n",
        "    \":(\" : \"حزين\",\n",
        "    \"xD\" : \"يضحك\",\n",
        "    \":=(\": \"يبكي\",\n",
        "    \":'(\": \"حزن\",\n",
        "    \":'‑(\": \"حزن\",\n",
        "    \"XD\" : \"يضحك\",\n",
        "    \":D\" : \"يبتسم\",\n",
        "    \"♬\" : \"موسيقي\",\n",
        "    \"♡\" : \"حب\",\n",
        "    \"☻\"  : \"يبتسم\",\n",
        "}\n",
        "\n",
        "def replace_emojis(text):\n",
        "    pattern = re.compile('|'.join(re.escape(key) for key in emojis.keys()))\n",
        "    replaced_text = pattern.sub(lambda match: emojis[match.group(0)] + ' ', text)\n",
        "    return emoji.replace_emoji(replaced_text, '')\n",
        "\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: replace_emojis(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ir8Glj9EEsf"
      },
      "source": [
        "## Removing mentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UXgrvqiKEEsg"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر  طريق #الدمام × #الخبر ابعدواا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب  شاطئ فلامنغو ، أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg      حذر  طريق #الدمام × #الخبر ابعدواا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos     مع بعض الأصدقاء حب  شاطئ فلامنغو ، أروبا"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = r'@[\\w]+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AA7faVLeEEsh"
      },
      "source": [
        "## Removing links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hm75bZvGEEsh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر  طريق #الدمام × #الخبر ابعدواا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب  شاطئ فلامنغو ، أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg      حذر  طريق #الدمام × #الخبر ابعدواا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos     مع بعض الأصدقاء حب  شاطئ فلامنغو ، أروبا"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = r'https?://\\S+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_TMg71jAEEsn"
      },
      "source": [
        "## Remove foriegn words"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1EjRxMEEsn"
      },
      "source": [
        "The text includes english, japanese and words for other languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HraPS5_SEEso"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر  طريق #الدمام × #الخبر ابعدواا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب  شاطئ فلامنغو ، أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg      حذر  طريق #الدمام × #الخبر ابعدواا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos     مع بعض الأصدقاء حب  شاطئ فلامنغو ، أروبا"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = r'[a-zA-Z]+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_r3f6p9DEEsp"
      },
      "source": [
        "## Remove punctuations & special chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g2gtB69BEEsp"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر  طريق  الدمام    الخبر ابعدواا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب  شاطئ فلامنغو   أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg      حذر  طريق  الدمام    الخبر ابعدواا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos     مع بعض الأصدقاء حب  شاطئ فلامنغو   أروبا"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = r'[^\\w\\s\\u0600-\\u06FF]+|_|ﷺ|۩|⓵|؟|؛|۞|ﷻ|،| ٰ'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-BEPhEhYEEsq"
      },
      "source": [
        "## Remove consecutive characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pjEAl55ZEEsr"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر طريق الدمام الخبر ابعدوا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب شاطئ فلامنغو أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg            حذر طريق الدمام الخبر ابعدوا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos        مع بعض الأصدقاء حب شاطئ فلامنغو أروبا"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = r'(.)\\1+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, r'\\1', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove tatweel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_tatweel(document))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MZctqr8qEEsr"
      },
      "source": [
        "## Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xUfiKGlxEEst"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر طريق الدمام الخبر ابعدوا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب شاطئ فلامنغو أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg            حذر طريق الدمام الخبر ابعدوا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos        مع بعض الأصدقاء حب شاطئ فلامنغو أروبا"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = r'\\d+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove extra whitespaces\n",
        "In this step we get rid of extra whitespaces as well as new lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر طريق الدمام الخبر ابعدوا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب شاطئ فلامنغو أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg            حذر طريق الدمام الخبر ابعدوا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos        مع بعض الأصدقاء حب شاطئ فلامنغو أروبا"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = r'\\s+|\\n+'\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, ' ', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove harakat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر طريق الدمام الخبر ابعدوا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب شاطئ فلامنغو أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg            حذر طريق الدمام الخبر ابعدوا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos        مع بعض الأصدقاء حب شاطئ فلامنغو أروبا"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_tashkeel(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove diactrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر طريق الدمام الخبر ابعدوا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الأصدقاء حب شاطئ فلامنغو أروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg            حذر طريق الدمام الخبر ابعدوا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos        مع بعض الأصدقاء حب شاطئ فلامنغو أروبا"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.strip_diacritics(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize hamza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>بخاخ فيجا الالماني انتصاب وتاخير دون تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور و يوم تعال خذ الزرقاء من فريق التماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر طريق الدمام الخبر ابعدوا عنه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلى صباح يضحك</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>مع بعض الاصدقاء حب شاطئ فلامنغو اروبا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                        tweet\n",
              "33372   pos   بخاخ فيجا الالماني انتصاب وتاخير دون تخدير\n",
              "41159   pos  شهور و يوم تعال خذ الزرقاء من فريق التماسيح\n",
              "18029   neg            حذر طريق الدمام الخبر ابعدوا عنه \n",
              "37264   pos                              احلى صباح يضحك \n",
              "35690   pos        مع بعض الاصدقاء حب شاطئ فلامنغو اروبا"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = r\"أ|إ|آ\"\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: re.sub(pattern, 'ا', document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sy4GggUEEsw"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_Nf_D_MeEEsx"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>[بخاخ, فيجا, الالماني, انتصاب, وتاخير, دون, تخ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>[شهور, و, يوم, تعال, خذ, الزرقاء, من, فريق, ال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>[حذر, طريق, الدمام, الخبر, ابعدوا, عنه]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>[احلى, صباح, يضحك]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>[مع, بعض, الاصدقاء, حب, شاطئ, فلامنغو, اروبا]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                              tweet\n",
              "33372   pos  [بخاخ, فيجا, الالماني, انتصاب, وتاخير, دون, تخ...\n",
              "41159   pos  [شهور, و, يوم, تعال, خذ, الزرقاء, من, فريق, ال...\n",
              "18029   neg            [حذر, طريق, الدمام, الخبر, ابعدوا, عنه]\n",
              "37264   pos                                 [احلى, صباح, يضحك]\n",
              "35690   pos      [مع, بعض, الاصدقاء, حب, شاطئ, فلامنغو, اروبا]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: araby.tokenize(document))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove long & short words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>[بخاخ, فيجا, الالماني, انتصاب, وتاخير, دون, تخ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>[شهور, يوم, تعال, خذ, الزرقاء, من, فريق, التما...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>[حذر, طريق, الدمام, الخبر, ابعدوا, عنه]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>[احلى, صباح, يضحك]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>[مع, بعض, الاصدقاء, حب, شاطئ, فلامنغو, اروبا]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                              tweet\n",
              "33372   pos  [بخاخ, فيجا, الالماني, انتصاب, وتاخير, دون, تخ...\n",
              "41159   pos  [شهور, يوم, تعال, خذ, الزرقاء, من, فريق, التما...\n",
              "18029   neg            [حذر, طريق, الدمام, الخبر, ابعدوا, عنه]\n",
              "37264   pos                                 [احلى, صباح, يضحك]\n",
              "35690   pos      [مع, بعض, الاصدقاء, حب, شاطئ, فلامنغو, اروبا]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: [word for word in document if len(word) < 9 and len(word) > 1])\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pQOiy36-EEsy"
      },
      "source": [
        "## Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hFgkKc6GEEsy"
      },
      "outputs": [],
      "source": [
        "ar_stemmer = stemmer(\"arabic\")\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda doc: [ar_stemmer.stemWord(token) for token in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>[خاخ, فيج, المان, انتصاب, تاخير, دون, تخدير]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>[شهور, يوم, تعال, خذ, زرقاء, من, ريق, تماسيح]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>[حذر, طريق, دمام, خبر, ابعد, عنه]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>[احلي, صباح, يضح]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>[مع, بعض, اصدقاء, حب, شاطء, امنغ, اروب]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                                          tweet\n",
              "33372   pos   [خاخ, فيج, المان, انتصاب, تاخير, دون, تخدير]\n",
              "41159   pos  [شهور, يوم, تعال, خذ, زرقاء, من, ريق, تماسيح]\n",
              "18029   neg              [حذر, طريق, دمام, خبر, ابعد, عنه]\n",
              "37264   pos                              [احلي, صباح, يضح]\n",
              "35690   pos        [مع, بعض, اصدقاء, حب, شاطء, امنغ, اروب]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "arabic_stopwords = stopwords.words('arabic')\n",
        "arabic_stopwords.extend(stp.stopwords_list())\n",
        "stop_words = {ar_stemmer.stemWord(entry) for entry in arabic_stopwords}\n",
        "with open(\"arabic_stopwords.txt\", \"r\", encoding=\"UTF-8\") as file:\n",
        "    for word in file:\n",
        "        stop_words.add(ar_stemmer.stemWord(word.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19241"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "open(\"./models/stopwords.pkl\", \"wb\").write(pickle.dumps(stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>33372</th>\n",
              "      <td>pos</td>\n",
              "      <td>خاخ فيج المان انتصاب تاخير تخدير</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41159</th>\n",
              "      <td>pos</td>\n",
              "      <td>شهور تعال خذ زرقاء ريق تماسيح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18029</th>\n",
              "      <td>neg</td>\n",
              "      <td>حذر طريق دمام ابعد</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37264</th>\n",
              "      <td>pos</td>\n",
              "      <td>احلي يضح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35690</th>\n",
              "      <td>pos</td>\n",
              "      <td>اصدقاء حب شاطء امنغ اروب</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      class                             tweet\n",
              "33372   pos  خاخ فيج المان انتصاب تاخير تخدير\n",
              "41159   pos     شهور تعال خذ زرقاء ريق تماسيح\n",
              "18029   neg                حذر طريق دمام ابعد\n",
              "37264   pos                          احلي يضح\n",
              "35690   pos          اصدقاء حب شاطء امنغ اروب"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def remove_stopwords(document: str) -> str:\n",
        "    words = set(document.split(\" \"))\n",
        "    return \" \".join(list(words - stop_words))\n",
        "\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(lambda document: \" \".join([token for token in document if token not in stop_words]))\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lw0Z0MsXEEsz"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"preprocessed_data.csv\"):\n",
        "    # remove empty entries\n",
        "    # data.replace('', pd.NA, inplace=True)  # Replace empty strings with NA\n",
        "    # data.dropna(inplace=True)  # Drop rows with NA values\n",
        "    data.to_csv(\"preprocessed_data.csv\") # inspect the resulting file to validate the preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l4AEbbxhEEsz"
      },
      "source": [
        "# Text representation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BoW (Bag-of-Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KnE8RuVGEEtI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(24742,)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify = data[\"class\"])\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "365668"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "open(\"./models/ASTC/BOW/vectorizer.pkl\", 'wb').write(pickle.dumps(vectorizer))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_N8hCmIEEtK",
        "outputId": "7fd72e2e-c809-445a-851d-035c5e8381eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 87.26%\n",
            "Precesion : 84.78%\n",
            "Recall : 90.15%\n",
            "F1 score : 87.38%\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"./models/ASTC/BOW/LR.pkl\"):\n",
        "    # read model from disk\n",
        "    model = open(\"./models/ASTC/BOW/LR.pkl\", 'rb').read()\n",
        "    model: LogisticRegression = pickle.loads(model)\n",
        "else:\n",
        "    model = LogisticRegression(random_state = SEED, max_iter = 1500)\n",
        "    model.fit(X_train, y_train)\n",
        "    # write model to disk\n",
        "    mdl_bytes = pickle.dumps(model)\n",
        "    open(\"./models/ASTC/BOW/LR.pkl\", 'wb').write(mdl_bytes)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM\n",
        "\n",
        "We were unable to train this model on our machines using the initial dataset, due to the **curse of dimensionality**, so we added dimensioanlity reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "pca = PCA(n_components = 150, random_state = SEED)\n",
        "svm = SVC(random_state = SEED)\n",
        "\n",
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)\n",
        "X_train_SVM = pca.fit_transform(scaled_X_train)\n",
        "X_test_SVM  = pca.transform(scaled_X_test)\n",
        "\n",
        "svm.fit(X_train_SVM, y_train)\n",
        "y_pred = svm.predict(X_test_SVM)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label = \"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(random_state = SEED)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify= data[\"class\"])\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "open(\"./models/ASTC/TFIDF/vectorizer.pkl\", 'wb').write(pickle.dumps(vectorizer))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(\"./models/ASTC/TFIDF/LR.pkl\"):\n",
        "    # read model from disk\n",
        "    model = open(\"./models/ASTC/TFIDF/LR.pkl\", 'rb').read()\n",
        "    model: LogisticRegression = pickle.loads(model)\n",
        "else:\n",
        "    model = LogisticRegression(random_state = SEED, max_iter = 300)\n",
        "    model.fit(X_train, y_train)\n",
        "    # write model to disk\n",
        "    mdl_bytes = pickle.dumps(model)\n",
        "    open(\"./models/ASTC/TFIDF/LR.pkl\", 'wb').write(mdl_bytes)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "pca = PCA(n_components = 150, random_state = SEED)\n",
        "svm = SVC(random_state = SEED)\n",
        "\n",
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)\n",
        "X_train_SVM = pca.fit_transform(scaled_X_train)\n",
        "X_test_SVM  = pca.transform(scaled_X_test)\n",
        "\n",
        "svm.fit(X_train_SVM, y_train)\n",
        "y_pred = svm.predict(X_test_SVM)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label = \"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(random_state = SEED)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify = data[\"class\"])\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components = 170, random_state = SEED)\n",
        "lda.fit(X_train)\n",
        "X_train = lda.transform(X_train)\n",
        "X_test = lda.transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(random_state = SEED)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "pca = PCA(n_components = 150, random_state = SEED)\n",
        "svm = SVC(random_state = SEED)\n",
        "\n",
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)\n",
        "X_train_SVM = pca.fit_transform(scaled_X_train)\n",
        "X_test_SVM  = pca.transform(scaled_X_test)\n",
        "\n",
        "svm.fit(X_train_SVM, y_train)\n",
        "y_pred = svm.predict(X_test_SVM)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label = \"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(random_state = SEED)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"class\"], test_size = 0.2, random_state = SEED, stratify = data[\"class\"])\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "lsa = TruncatedSVD(n_components = 200, random_state = SEED)\n",
        "X_train = lsa.fit_transform(X_train)\n",
        "X_test = lsa.transform(X_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 58.59%\n",
            "Precesion : 55.65%\n",
            "Recall : 75.55%\n",
            "F1 score : 64.10%\n"
          ]
        }
      ],
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(random_state = SEED)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "pca = PCA(n_components = 150, random_state = SEED)\n",
        "svm = SVC(random_state = SEED)\n",
        "\n",
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)\n",
        "X_train_SVM = pca.fit_transform(scaled_X_train)\n",
        "X_test_SVM  = pca.transform(scaled_X_test)\n",
        "\n",
        "svm.fit(X_train_SVM, y_train)\n",
        "y_pred = svm.predict(X_test_SVM)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label = \"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BoC (Bag-of-concepts)\n",
        "\n",
        "The BOC (Bag-of-Concepts) method has been proposed as a solution to the problem of large dimensions and sparsity that traditional methods such as TF-IDF and Bag of words suffer from."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word embeddings\n",
        "This is done using the AraVec model which is trained on arabic tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec.load(\"./aravec/full_uni_cbow_100_twitter/full_uni_cbow_100_twitter.mdl\")\n",
        "word_vecs = {}\n",
        "total, skipped = 0, 0\n",
        "for tweet in data[\"tweet\"]:\n",
        "    for word in tweet.split(\" \"):\n",
        "        total += 1\n",
        "        try:\n",
        "            word_vecs[word] = model.wv[word]\n",
        "        except Exception:\n",
        "            skipped += 1\n",
        "print(f\"total skipped : {skipped} ({(skipped / total) * 100 :.2f}%)\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clustering the words embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_CONCEPTS = 130\n",
        "model = KMeans(n_clusters = NUM_CONCEPTS, random_state = SEED)\n",
        "X = list(word_vecs.values())\n",
        "model.fit(X)\n",
        "concepts = model.predict(X)\n",
        "print(len(X))"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Concept extraction\n",
        "Now that we related words to concepts, we can create a document representataion, in which we express the degree of which a document contains a certain concept, and instead of taking only the freuqncies, we consider an approach similar to TF-IDF called, CF-IDF.\n",
        "\n",
        "CF-IDF is defined using the following formula : \n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "such that : \n",
        "\n",
        "    * |D| is the number of documents in the corpus\n",
        "    * n_c is the number of occurences of concept c in document d\n",
        "    * n_k is the total number of concepts in this document "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_DOCS = data.shape[0]\n",
        "\n",
        "# construct a word to concept mapping\n",
        "word_concept = {}\n",
        "for index, word in enumerate(word_vecs.keys()):\n",
        "    word_concept[word] = concepts[index]\n",
        "print(word_concept)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# construct a concept to document count mapping\n",
        "concept_docs = defaultdict(int)\n",
        "for doc in data[\"tweet\"]:\n",
        "    doc_concepts = set()\n",
        "    for word in doc.split(\" \"):\n",
        "        try:\n",
        "            doc_concepts.add(word_concept[word])\n",
        "        except Exception:\n",
        "            pass\n",
        "    for concept in doc_concepts:\n",
        "        concept_docs[concept] += 1\n",
        "print(concept_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cf_idf(document: str):\n",
        "    \"\"\" Returns the CD-IDF representataion of a document \"\"\"\n",
        "    res = [0 for _ in range(NUM_CONCEPTS)]\n",
        "    concepts_counts = defaultdict(int)\n",
        "    for word in document.split(\" \"):\n",
        "        try:\n",
        "            concepts_counts[word_concept[word]] += 1\n",
        "        except:\n",
        "            pass\n",
        "    n_k = sum(concepts_counts.values()) # number of concepts present in the document (duplicates are considered!)\n",
        "    for concept in range(NUM_CONCEPTS):\n",
        "        if concepts_counts[concept] != 0:\n",
        "            res[concept] = (concepts_counts[concept] / n_k) * log(NUM_DOCS / (1 + concept_docs[concept]))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = [cf_idf(tweet) for tweet in data[\"tweet\"]]\n",
        "y = data[\"class\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = SEED, stratify = y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation\n",
        "#### Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GaussianNB()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(random_state = SEED)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SVC(random_state = SEED)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(random_state = SEED)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=\"pos\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precesion : {precision * 100:.2f}%\")\n",
        "print(f\"Recall : {recall * 100:.2f}%\")\n",
        "print(f\"F1 score : {f1_score * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
